

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Bayesian Optimization &mdash; Pyro Tutorials 编译 1.3.0 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script type="text/javascript" src="_static/jquery.js"></script>
        <script type="text/javascript" src="_static/underscore.js"></script>
        <script type="text/javascript" src="_static/doctools.js"></script>
        <script type="text/javascript" src="_static/language_data.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/pyro.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Writing guides using EasyGuide" href="easyguide.html" />
    <link rel="prev" title="Gaussian Process Latent Variable Model" href="gplvm.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html">
          

          
            
            <img src="_static/pyro_logo_wide.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                1.3.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Introduction:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="intro_part_i.html">Pyro 模型简介</a></li>
<li class="toctree-l1"><a class="reference internal" href="intro_part_ii.html">Pyro 推断简介</a></li>
<li class="toctree-l1"><a class="reference internal" href="svi_part_i.html">SVI Part I: Pyro 随机变分推断基础</a></li>
<li class="toctree-l1"><a class="reference internal" href="svi_part_ii.html">SVI Part II: 条件独立, 子采样和 Amortization</a></li>
<li class="toctree-l1"><a class="reference internal" href="svi_part_iii.html">SVI Part III: ELBO Gradient Estimators</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_shapes.html">Tensor shapes in Pyro</a></li>
</ul>
<p class="caption"><span class="caption-text">Advanced:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="enumeration.html">Inference with Discrete Latent Variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="custom_objectives.html">自定义 SVI 目标函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="jit.html">Pyro 模型中使用 PyTorch JIT Compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="minipyro.html">Mini-Pyro</a></li>
<li class="toctree-l1"><a class="reference internal" href="effect_handlers.html">Poutine: Pyro 中使用 Effect Handlers 编程手册</a></li>
</ul>
<p class="caption"><span class="caption-text">Examples:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="vae.html">变分自编码器</a></li>
<li class="toctree-l1"><a class="reference internal" href="bayesian_regression.html">贝叶斯回归- Introduction (Part 1)</a></li>
<li class="toctree-l1"><a class="reference internal" href="bayesian_regression_ii.html">贝叶斯回归-推断算法(Part 2)</a></li>
<li class="toctree-l1"><a class="reference internal" href="dmm.html">Deep Markov Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="air.html">Attend Infer Repeat</a></li>
<li class="toctree-l1"><a class="reference internal" href="ss-vae.html">半监督 VAE</a></li>
<li class="toctree-l1"><a class="reference internal" href="stable.html">随机波动率的 Levy 稳定分布模型</a></li>
</ul>
<p class="caption"><span class="caption-text">Contributed:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="gmm.html">Gaussian Mixture Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="gp.html">Gaussian Processes</a></li>
<li class="toctree-l1"><a class="reference internal" href="gplvm.html">Gaussian Process Latent Variable Model</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Bayesian Optimization</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#Problem-Setup">Problem Setup</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Define-an-objective-function">Define an objective function</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Setting-a-Gaussian-Process-prior">Setting a Gaussian Process prior</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Define-an-acquisition-function">Define an acquisition function</a></li>
<li class="toctree-l2"><a class="reference internal" href="#The-inner-loop-of-Bayesian-Optimization">The inner loop of Bayesian Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Running-the-algorithm">Running the algorithm</a></li>
<li class="toctree-l2"><a class="reference internal" href="#References">References</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="easyguide.html">Writing guides using EasyGuide</a></li>
<li class="toctree-l1"><a class="reference internal" href="forecasting_i.html">Forecasting I: univariate, heavy tailed</a></li>
<li class="toctree-l1"><a class="reference internal" href="forecasting_ii.html">Forecasting II: state space models</a></li>
<li class="toctree-l1"><a class="reference internal" href="forecasting_iii.html">Forecasting III: hierarchical models</a></li>
<li class="toctree-l1"><a class="reference internal" href="tracking_1d.html">Tracking an Unknown Number of Objects</a></li>
<li class="toctree-l1"><a class="reference internal" href="csis.html">Compiled Sequential Importance Sampling</a></li>
<li class="toctree-l1"><a class="reference internal" href="RSA-implicature.html">The Rational Speech Act framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="RSA-hyperbole.html">Understanding Hyperbole using RSA</a></li>
<li class="toctree-l1"><a class="reference internal" href="ekf.html">Kalman Filter</a></li>
<li class="toctree-l1"><a class="reference internal" href="working_memory.html">Designing Adaptive Experiments to Study Working Memory</a></li>
<li class="toctree-l1"><a class="reference internal" href="elections.html">Predicting the outcome of a US presidential election using Bayesian optimal experimental design</a></li>
<li class="toctree-l1"><a class="reference internal" href="dirichlet_process_mixture.html">Dirichlet Process Mixture Models in Pyro</a></li>
<li class="toctree-l1"><a class="reference internal" href="boosting_bbvi.html">Boosting Black Box Variational Inference</a></li>
</ul>
<p class="caption"><span class="caption-text">Code Examples:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="capture_recapture.html">Capture-Recapture Models (CJS Models)</a></li>
<li class="toctree-l1"><a class="reference internal" href="cevae.html">Causal Effect VAE</a></li>
<li class="toctree-l1"><a class="reference internal" href="hmm.html">Hidden Markov Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="lda.html">Latent Dirichlet Allocation</a></li>
<li class="toctree-l1"><a class="reference internal" href="mcmc.html">Markov Chain Monte Carlo</a></li>
<li class="toctree-l1"><a class="reference internal" href="neutra.html">NeuTraReparam</a></li>
<li class="toctree-l1"><a class="reference internal" href="sparse_gamma.html">Sparse Gamma Deep Exponential Family</a></li>
<li class="toctree-l1"><a class="reference internal" href="dkl.html">Deep Kernel Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="einsum.html">Plated Einsum</a></li>
<li class="toctree-l1"><a class="reference internal" href="forecast_simple.html">Multivariate Forecasting</a></li>
<li class="toctree-l1"><a class="reference internal" href="timeseries.html">Gaussian Process Time Series Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="smcfilter.html">Sequential Monte Carlo Filtering</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Pyro Tutorials 编译</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>Bayesian Optimization</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/bo.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container,
div.nbinput.container div.prompt,
div.nbinput.container div.input_area,
div.nbinput.container div[class*=highlight],
div.nbinput.container div[class*=highlight] pre,
div.nboutput.container,
div.nboutput.container div.prompt,
div.nboutput.container div.output_area,
div.nboutput.container div[class*=highlight],
div.nboutput.container div[class*=highlight] pre {
    background: none;
    border: none;
    padding: 0 0;
    margin: 0;
    box-shadow: none;
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    min-width: 5ex;
    padding-top: 0.3rem;
    padding-right: 0.3rem;
    text-align: right;
    flex: 0;
}
@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    background: #f5f5f5;
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 0.3rem;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt a.copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="Bayesian-Optimization">
<h1>Bayesian Optimization<a class="headerlink" href="#Bayesian-Optimization" title="Permalink to this headline">¶</a></h1>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Bayesian_optimization">Bayesian optimization</a> is a powerful strategy for minimizing (or maximizing) objective functions that are costly to evaluate. It is an important component of <a class="reference external" href="https://en.wikipedia.org/wiki/Automated_machine_learning">automated machine learning</a> toolboxes such as <a class="reference external" href="https://automl.github.io/auto-sklearn/stable/">auto-sklearn</a>, <a class="reference external" href="http://www.cs.ubc.ca/labs/beta/Projects/autoweka/">auto-weka</a>, and
<a class="reference external" href="https://scikit-optimize.github.io/">scikit-optimize</a>, where Bayesian optimization is used to select model hyperparameters. Bayesian optimization is used for a wide range of other applications as well; as cataloged in the review [2], these include interactive user-interfaces, robotics, environmental monitoring, information extraction, combinatorial optimization, sensor networks, adaptive Monte Carlo, experimental design, and reinforcement learning.</p>
<div class="section" id="Problem-Setup">
<h2>Problem Setup<a class="headerlink" href="#Problem-Setup" title="Permalink to this headline">¶</a></h2>
<p>We are given a minimization problem</p>
<div class="math notranslate nohighlight">
\[x^* = \text{arg}\min \ f(x),\]</div>
<p>where <span class="math notranslate nohighlight">\(f\)</span> is a fixed objective function that we can evaluate pointwise. Here we assume that we do <em>not</em> have access to the gradient of <span class="math notranslate nohighlight">\(f\)</span>. We also allow for the possibility that evaluations of <span class="math notranslate nohighlight">\(f\)</span> are noisy.</p>
<p>To solve the minimization problem, we will construct a sequence of points <span class="math notranslate nohighlight">\(\{x_n\}\)</span> that converge to <span class="math notranslate nohighlight">\(x^*\)</span>. Since we implicitly assume that we have a fixed budget (say 100 evaluations), we do not expect to find the exact minumum <span class="math notranslate nohighlight">\(x^*\)</span>: the goal is to get the best approximate solution we can given the allocated budget.</p>
<p>The Bayesian optimization strategy works as follows:</p>
<ol class="arabic">
<li><p>Place a prior on the objective function <span class="math notranslate nohighlight">\(f\)</span>. Each time we evaluate <span class="math notranslate nohighlight">\(f\)</span> at a new point <span class="math notranslate nohighlight">\(x_n\)</span>, we update our model for <span class="math notranslate nohighlight">\(f(x)\)</span>. This model serves as a surrogate objective function and reflects our beliefs about <span class="math notranslate nohighlight">\(f\)</span> (in particular it reflects our beliefs about where we expect <span class="math notranslate nohighlight">\(f(x)\)</span> to be close to <span class="math notranslate nohighlight">\(f(x^*)\)</span>). Since we are being Bayesian, our beliefs are encoded in a posterior that allows us to systematically reason about the uncertainty of our model
predictions.</p></li>
<li><p>Use the posterior to derive an “acquisition” function <span class="math notranslate nohighlight">\(\alpha(x)\)</span> that is easy to evaluate and differentiate (so that optimizing <span class="math notranslate nohighlight">\(\alpha(x)\)</span> is easy). In contrast to <span class="math notranslate nohighlight">\(f(x)\)</span>, we will generally evaluate <span class="math notranslate nohighlight">\(\alpha(x)\)</span> at many points <span class="math notranslate nohighlight">\(x\)</span>, since doing so will be cheap.</p></li>
<li><p>Repeat until convergence:</p>
<ul>
<li><p>Use the acquisition function to derive the next query point according to</p>
<div class="math notranslate nohighlight">
\[x_{n+1} = \text{arg}\min \ \alpha(x).\]</div>
</li>
<li><p>Evaluate <span class="math notranslate nohighlight">\(f(x_{n+1})\)</span> and update the posterior.</p></li>
</ul>
</li>
</ol>
<p>A good acquisition function should make use of the uncertainty encoded in the posterior to encourage a balance between exploration—querying points where we know little about <span class="math notranslate nohighlight">\(f\)</span>—and exploitation—querying points in regions we have good reason to think <span class="math notranslate nohighlight">\(x^*\)</span> may lie. As the iterative procedure progresses our model for <span class="math notranslate nohighlight">\(f\)</span> evolves and so does the acquisition function. If our model is good and we’ve chosen a reasonable acquisition function, we expect that the acquisition function
will guide the query points <span class="math notranslate nohighlight">\(x_n\)</span> towards <span class="math notranslate nohighlight">\(x^*\)</span>.</p>
<p>In this tutorial, our model for <span class="math notranslate nohighlight">\(f\)</span> will be a Gaussian process. In particular we will see how to use the <a class="reference external" href="http://docs.pyro.ai/en/0.3.1/contrib.gp.html">Gaussian Process module</a> in Pyro to implement a simple Bayesian optimization procedure.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">matplotlib.gridspec</span> <span class="k">as</span> <span class="nn">gridspec</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.autograd</span> <span class="k">as</span> <span class="nn">autograd</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
<span class="kn">from</span> <span class="nn">torch.distributions</span> <span class="k">import</span> <span class="n">constraints</span><span class="p">,</span> <span class="n">transform_to</span>

<span class="kn">import</span> <span class="nn">pyro</span>
<span class="kn">import</span> <span class="nn">pyro.contrib.gp</span> <span class="k">as</span> <span class="nn">gp</span>

<span class="k">assert</span> <span class="n">pyro</span><span class="o">.</span><span class="n">__version__</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;1.3.0&#39;</span><span class="p">)</span>
<span class="n">pyro</span><span class="o">.</span><span class="n">enable_validation</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># can help with debugging</span>
<span class="n">pyro</span><span class="o">.</span><span class="n">set_rng_seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Define-an-objective-function">
<h2>Define an objective function<a class="headerlink" href="#Define-an-objective-function" title="Permalink to this headline">¶</a></h2>
<p>For the purposes of demonstration, the objective function we are going to consider is the <a class="reference external" href="https://www.sfu.ca/~ssurjano/forretal08.html">Forrester et al. (2008) function</a>:</p>
<div class="math notranslate nohighlight">
\[f(x) = (6x-2)^2 \sin(12x-4), \quad x\in [0, 1].\]</div>
<p>This function has both a local minimum and a global minimum. The global minimum is at <span class="math notranslate nohighlight">\(x^* = 0.75725\)</span>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="mi">6</span> <span class="o">*</span> <span class="n">x</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">12</span> <span class="o">*</span> <span class="n">x</span> <span class="o">-</span> <span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Let’s begin by plotting <span class="math notranslate nohighlight">\(f\)</span>.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/bo_5_0.png" src="_images/bo_5_0.png" />
</div>
</div>
</div>
<div class="section" id="Setting-a-Gaussian-Process-prior">
<h2>Setting a Gaussian Process prior<a class="headerlink" href="#Setting-a-Gaussian-Process-prior" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Gaussian_process">Gaussian processes</a> are a popular choice for a function priors due to their power and flexibility. The core of a Gaussian Process is its covariance function <span class="math notranslate nohighlight">\(k\)</span>, which governs the similarity of <span class="math notranslate nohighlight">\(f(x)\)</span> for pairs of input points. Here we will use a Gaussian Process as our prior for the objective function <span class="math notranslate nohighlight">\(f\)</span>. Given inputs <span class="math notranslate nohighlight">\(X\)</span> and the corresponding noisy observations <span class="math notranslate nohighlight">\(y\)</span>, the model takes the form</p>
<div class="math notranslate nohighlight">
\[f\sim\mathrm{MultivariateNormal}(0,k(X,X)),\]</div>
<div class="math notranslate nohighlight">
\[y\sim f+\epsilon,\]</div>
<p>where <span class="math notranslate nohighlight">\(\epsilon\)</span> is i.i.d. Gaussian noise and <span class="math notranslate nohighlight">\(k(X,X)\)</span> is a covariance matrix whose entries are given by <span class="math notranslate nohighlight">\(k(x,x^\prime)\)</span> for each pair of inputs <span class="math notranslate nohighlight">\((x,x^\prime)\)</span>.</p>
<p>We choose the <a class="reference external" href="https://en.wikipedia.org/wiki/Mat%C3%A9rn_covariance_function">Matern</a> kernel with <span class="math notranslate nohighlight">\(\nu = \frac{5}{2}\)</span> (as suggested in reference [1]). Note that the popular <a class="reference external" href="https://en.wikipedia.org/wiki/Radial_basis_function_kernel">RBF</a> kernel, which is used in many regression tasks, results in a function prior whose samples are infinitely differentiable; this is probably an unrealistic assumption for most ‘black-box’ objective functions.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># initialize the model with four input points: 0.0, 0.33, 0.66, 1.0</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.33</span><span class="p">,</span> <span class="mf">0.66</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">gpmodel</span> <span class="o">=</span> <span class="n">gp</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">GPRegression</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">gp</span><span class="o">.</span><span class="n">kernels</span><span class="o">.</span><span class="n">Matern52</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
                                 <span class="n">noise</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.1</span><span class="p">),</span> <span class="n">jitter</span><span class="o">=</span><span class="mf">1.0e-4</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>The following helper function <code class="docutils literal notranslate"><span class="pre">update_posterior</span></code> will take care of updating our <code class="docutils literal notranslate"><span class="pre">gpmodel</span></code> each time we evaluate <span class="math notranslate nohighlight">\(f\)</span> at a new value <span class="math notranslate nohighlight">\(x\)</span>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">update_posterior</span><span class="p">(</span><span class="n">x_new</span><span class="p">):</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x_new</span><span class="p">)</span> <span class="c1"># evaluate f at new point.</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">gpmodel</span><span class="o">.</span><span class="n">X</span><span class="p">,</span> <span class="n">x_new</span><span class="p">])</span> <span class="c1"># incorporate new evaluation</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">gpmodel</span><span class="o">.</span><span class="n">y</span><span class="p">,</span> <span class="n">y</span><span class="p">])</span>
    <span class="n">gpmodel</span><span class="o">.</span><span class="n">set_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="c1"># optimize the GP hyperparameters using Adam with lr=0.001</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">gpmodel</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
    <span class="n">gp</span><span class="o">.</span><span class="n">util</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">gpmodel</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Define-an-acquisition-function">
<h2>Define an acquisition function<a class="headerlink" href="#Define-an-acquisition-function" title="Permalink to this headline">¶</a></h2>
<p>There are many reasonable options for the acquisition function (see references [1] and [2] for a list of popular choices and a discussion of their properties). Here we will use one that is ‘simple to implement and interpret,’ namely the ‘Lower Confidence Bound’ acquisition function. It is given by</p>
<div class="math notranslate nohighlight">
\[\alpha(x) = \mu(x) - \kappa \sigma(x)\]</div>
<p>where <span class="math notranslate nohighlight">\(\mu(x)\)</span> and <span class="math notranslate nohighlight">\(\sigma(x)\)</span> are the mean and square root variance of the posterior at the point <span class="math notranslate nohighlight">\(x\)</span>, and the arbitrary constant <span class="math notranslate nohighlight">\(\kappa&gt;0\)</span> controls the trade-off between exploitation and exploration. This acquisition function will be minimized for choices of <span class="math notranslate nohighlight">\(x\)</span> where either: i) <span class="math notranslate nohighlight">\(\mu(x)\)</span> is small (exploitation); or ii) where <span class="math notranslate nohighlight">\(\sigma(x)\)</span> is large (exploration). A large value of <span class="math notranslate nohighlight">\(\kappa\)</span> means that we place more weight on exploration because we
prefer candidates <span class="math notranslate nohighlight">\(x\)</span> in areas of high uncertainty. A small value of <span class="math notranslate nohighlight">\(\kappa\)</span> encourages exploitation because we prefer candidates <span class="math notranslate nohighlight">\(x\)</span> that minimize <span class="math notranslate nohighlight">\(\mu(x)\)</span>, which is the mean of our surrogate objective function. We will use <span class="math notranslate nohighlight">\(\kappa=2\)</span>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">lower_confidence_bound</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">kappa</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
    <span class="n">mu</span><span class="p">,</span> <span class="n">variance</span> <span class="o">=</span> <span class="n">gpmodel</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">full_cov</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">noiseless</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">variance</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">mu</span> <span class="o">-</span> <span class="n">kappa</span> <span class="o">*</span> <span class="n">sigma</span>
</pre></div>
</div>
</div>
<p>The final component we need is a way to find (approximate) minimizing points <span class="math notranslate nohighlight">\(x_{\rm min}\)</span> of the acquisition function. There are several ways to proceed, including gradient-based and non-gradient-based techniques. Here we will follow the gradient-based approach. One of the possible drawbacks of gradient descent methods is that the minimization algorithm can get stuck at a local minimum. In this tutorial, we adopt a (very) simple approach to address this issue:</p>
<ul class="simple">
<li><p>First, we seed our minimization algorithm with 5 different values: i) one is chosen to be <span class="math notranslate nohighlight">\(x_{n-1}\)</span>, i.e. the candidate <span class="math notranslate nohighlight">\(x\)</span> used in the previous step; and ii) four are chosen uniformly at random from the domain of the objective function.</p></li>
<li><p>We then run the minimization algorithm to approximate convergence for each seed value.</p></li>
<li><p>Finally, from the five candidate <span class="math notranslate nohighlight">\(x\)</span>s identified by the minimization algorithm, we select the one that minimizes the acquisition function.</p></li>
</ul>
<p>Please refer to reference [2] for a more detailed discussion of this problem in Bayesian Optimization.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">find_a_candidate</span><span class="p">(</span><span class="n">x_init</span><span class="p">,</span> <span class="n">lower_bound</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">upper_bound</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="c1"># transform x to an unconstrained domain</span>
    <span class="n">constraint</span> <span class="o">=</span> <span class="n">constraints</span><span class="o">.</span><span class="n">interval</span><span class="p">(</span><span class="n">lower_bound</span><span class="p">,</span> <span class="n">upper_bound</span><span class="p">)</span>
    <span class="n">unconstrained_x_init</span> <span class="o">=</span> <span class="n">transform_to</span><span class="p">(</span><span class="n">constraint</span><span class="p">)</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">x_init</span><span class="p">)</span>
    <span class="n">unconstrained_x</span> <span class="o">=</span> <span class="n">unconstrained_x_init</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">minimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">LBFGS</span><span class="p">([</span><span class="n">unconstrained_x</span><span class="p">],</span> <span class="n">line_search_fn</span><span class="o">=</span><span class="s1">&#39;strong_wolfe&#39;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">closure</span><span class="p">():</span>
        <span class="n">minimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">transform_to</span><span class="p">(</span><span class="n">constraint</span><span class="p">)(</span><span class="n">unconstrained_x</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">lower_confidence_bound</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">autograd</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">unconstrained_x</span><span class="p">,</span> <span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">unconstrained_x</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">y</span>

    <span class="n">minimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">closure</span><span class="p">)</span>
    <span class="c1"># after finding a candidate in the unconstrained domain,</span>
    <span class="c1"># convert it back to original domain.</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">transform_to</span><span class="p">(</span><span class="n">constraint</span><span class="p">)(</span><span class="n">unconstrained_x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="The-inner-loop-of-Bayesian-Optimization">
<h2>The inner loop of Bayesian Optimization<a class="headerlink" href="#The-inner-loop-of-Bayesian-Optimization" title="Permalink to this headline">¶</a></h2>
<p>With the various helper functions defined above, we can now encapsulate the main logic of a single step of Bayesian Optimization in the function <code class="docutils literal notranslate"><span class="pre">next_x</span></code>:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">next_x</span><span class="p">(</span><span class="n">lower_bound</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">upper_bound</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_candidates</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
    <span class="n">candidates</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">values</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="n">x_init</span> <span class="o">=</span> <span class="n">gpmodel</span><span class="o">.</span><span class="n">X</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">:]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_candidates</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">find_a_candidate</span><span class="p">(</span><span class="n">x_init</span><span class="p">,</span> <span class="n">lower_bound</span><span class="p">,</span> <span class="n">upper_bound</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">lower_confidence_bound</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">candidates</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">values</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="n">x_init</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">new_empty</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="n">lower_bound</span><span class="p">,</span> <span class="n">upper_bound</span><span class="p">)</span>

    <span class="n">argmin</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">values</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">candidates</span><span class="p">[</span><span class="n">argmin</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Running-the-algorithm">
<h2>Running the algorithm<a class="headerlink" href="#Running-the-algorithm" title="Permalink to this headline">¶</a></h2>
<p>To illustrate how Bayesian Optimization works, we make a convenient plotting function that will help us visualize our algorithm’s progress.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">plot</span><span class="p">(</span><span class="n">gs</span><span class="p">,</span> <span class="n">xmin</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">with_title</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="n">xlabel</span> <span class="o">=</span> <span class="s2">&quot;xmin&quot;</span> <span class="k">if</span> <span class="n">xlabel</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="s2">&quot;x</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">xlabel</span><span class="p">)</span>
    <span class="n">Xnew</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">)</span>
    <span class="n">ax1</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">gpmodel</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">gpmodel</span><span class="o">.</span><span class="n">y</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="s2">&quot;kx&quot;</span><span class="p">)</span>  <span class="c1"># plot all observed data</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">loc</span><span class="p">,</span> <span class="n">var</span> <span class="o">=</span> <span class="n">gpmodel</span><span class="p">(</span><span class="n">Xnew</span><span class="p">,</span> <span class="n">full_cov</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">noiseless</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">sd</span> <span class="o">=</span> <span class="n">var</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span>
        <span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">Xnew</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">loc</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="s2">&quot;r&quot;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># plot predictive mean</span>
        <span class="n">ax1</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">Xnew</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">loc</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="o">-</span> <span class="mi">2</span><span class="o">*</span><span class="n">sd</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">loc</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">sd</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span>
                         <span class="n">color</span><span class="o">=</span><span class="s2">&quot;C0&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>  <span class="c1"># plot uncertainty intervals</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Find </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">xlabel</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">with_title</span><span class="p">:</span>
        <span class="n">ax1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Gaussian Process Regression&quot;</span><span class="p">)</span>

    <span class="n">ax2</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="c1"># plot the acquisition function</span>
        <span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">Xnew</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">lower_confidence_bound</span><span class="p">(</span><span class="n">Xnew</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
        <span class="c1"># plot the new candidate point</span>
        <span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xmin</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">lower_confidence_bound</span><span class="p">(</span><span class="n">xmin</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="s2">&quot;^&quot;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                 <span class="n">label</span><span class="o">=</span><span class="s2">&quot;</span><span class="si">{}</span><span class="s2"> = </span><span class="si">{:.5f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">xlabel</span><span class="p">,</span> <span class="n">xmin</span><span class="o">.</span><span class="n">item</span><span class="p">()))</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">with_title</span><span class="p">:</span>
        <span class="n">ax2</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Acquisition Function&quot;</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Our surrogate model <code class="docutils literal notranslate"><span class="pre">gpmodel</span></code> already has 4 function evaluations at its disposal; however, we have yet to optimize the GP hyperparameters. So we do that first. Then in a loop we call the <code class="docutils literal notranslate"><span class="pre">next_x</span></code> and <code class="docutils literal notranslate"><span class="pre">update_posterior</span></code> functions repeatedly. The following plot illustrates how Gaussian Process posteriors and the corresponding acquisition functions change at each step in the algorith. Note how query points are chosen both for exploration and exploitation.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">30</span><span class="p">))</span>
<span class="n">outer_gs</span> <span class="o">=</span> <span class="n">gridspec</span><span class="o">.</span><span class="n">GridSpec</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">gpmodel</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
<span class="n">gp</span><span class="o">.</span><span class="n">util</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">gpmodel</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">8</span><span class="p">):</span>
    <span class="n">xmin</span> <span class="o">=</span> <span class="n">next_x</span><span class="p">()</span>
    <span class="n">gs</span> <span class="o">=</span> <span class="n">gridspec</span><span class="o">.</span><span class="n">GridSpecFromSubplotSpec</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">subplot_spec</span><span class="o">=</span><span class="n">outer_gs</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="n">plot</span><span class="p">(</span><span class="n">gs</span><span class="p">,</span> <span class="n">xmin</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">with_title</span><span class="o">=</span><span class="p">(</span><span class="n">i</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">))</span>
    <span class="n">update_posterior</span><span class="p">(</span><span class="n">xmin</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/bo_22_0.png" src="_images/bo_22_0.png" />
</div>
</div>
<p>Because we have assumed that our observations contain noise, it is improbable that we will find the exact minimizer of the function <span class="math notranslate nohighlight">\(f\)</span>. Still, with a relatively small budget of evaluations (8) we see that the algorithm has converged to very close to the global minimum at <span class="math notranslate nohighlight">\(x^* = 0.75725\)</span>.</p>
<p>While this tutorial is only intended to be a brief introduction to Bayesian Optimization, we hope that we have been able to convey the basic underlying ideas. Consider watching the lecture by Nando de Freitas [3] for an excellent exposition of the basic theory. Finally, the reference paper [2] gives a review of recent research on Bayesian Optimization, together with many discussions about important technical details.</p>
</div>
<div class="section" id="References">
<h2>References<a class="headerlink" href="#References" title="Permalink to this headline">¶</a></h2>
<p>[1] <code class="docutils literal notranslate"><span class="pre">Practical</span> <span class="pre">bayesian</span> <span class="pre">optimization</span> <span class="pre">of</span> <span class="pre">machine</span> <span class="pre">learning</span> <span class="pre">algorithms</span></code>,     Jasper Snoek, Hugo Larochelle, and Ryan P. Adams</p>
<p>[2] <code class="docutils literal notranslate"><span class="pre">Taking</span> <span class="pre">the</span> <span class="pre">human</span> <span class="pre">out</span> <span class="pre">of</span> <span class="pre">the</span> <span class="pre">loop:</span> <span class="pre">A</span> <span class="pre">review</span> <span class="pre">of</span> <span class="pre">bayesian</span> <span class="pre">optimization</span></code>,     Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P. Adams, and Nando De Freitas</p>
<p>[3] <a class="reference external" href="https://www.youtube.com/watch?v=vz3D36VXefI">Machine learning - Bayesian optimization and multi-armed bandits</a></p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="easyguide.html" class="btn btn-neutral float-right" title="Writing guides using EasyGuide" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="gplvm.html" class="btn btn-neutral float-left" title="Gaussian Process Latent Variable Model" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright Uber Technologies, Inc; 编译 by Heyang Gong

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>