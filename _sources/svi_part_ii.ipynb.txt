{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVI Part II: 条件独立, 子采样和 Amortization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **The Goal: Scaling SVI to Large Datasets**\n",
    "\n",
    "一般情况下 SVI 过程中每次更新的计算复杂度是正比于样本数，所以我们需要是用 mini-batch 的办法减少复杂度。For a model with $N$ observations, running the `model` and `guide` and constructing the ELBO involves evaluating log pdf's whose 计算复杂度随着样本数 $N$ 比例增长. This is a problem if we want to scale to large datasets. 幸运的是，目标函数 ELBO naturally 支持子采样 provided that 我们的 model/guide 具有一些条件独立性结构 that we can take advantage of. 例如, in the case that the observations are conditionally independent given the latents, the log likelihood term in the ELBO can be approximated with\n",
    "\n",
    "$$ E[\\log p({\\bf x}| {\\bf z})] \\approx \\frac{1}{N}\\sum_{i=1}^N \\log p({\\bf x}_i | {\\bf z}) \\approx  \\frac{1}{M}\n",
    "\\sum_{i\\in{\\mathcal{I}_M}} \\log p({\\bf x}_i | {\\bf z})  $$\n",
    "\n",
    "where $\\mathcal{I}_M$ is a mini-batch of indices of size $M$ with $M<N$ (for a discussion please see references [1,2]). \n",
    "很好，问题解决了！但是我们如何在Pyro中实现这一点？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回顾：\n",
    "\n",
    "- 因为 `model = primitive r.v. + deterministic function`, 那么 `guide` 就应该对应的是此公式中 `latent r.v.` 给定部分 `observed r.v.` 的后验分布。\n",
    "- 在变分自编码器中，用来近似后验分布的 `guide` $q_\\phi(z)$ 是局部的，意味着对一个每个样本 $x_i$, `latent r.v.` 的后验分布 $q_\\phi(z|x_i)$ 都不相同，我们需要学习与样本个数相同数量的后验分布。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 在Pyro中标记条件独立\n",
    "\n",
    "\n",
    "If a user wants to do this sort of thing in Pyro, 则首先需要确保 the model and guide are written in such a way that Pyro can leverage the relevant conditional independencies. Let's see how this is done. Pyro 提供了两种用于标记条件独立性的语言原语(language primitive): `plate` and `markov`. Let's start with the simpler of the two.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<big> `Pyro.plate`: 从 Sequential `plate` 到 Vectorized `plate` </big>\n",
    "\n",
    "让我们回到 [previous tutorial](svi_part_i.ipynb) 中使用的例子。为了方便起见，让我们在这里回顾 `model` 的主要逻辑：\n",
    "\n",
    "```python\n",
    "def model(data):\n",
    "    # sample f from the beta prior\n",
    "    f = pyro.sample(\"latent_fairness\", dist.Beta(alpha0, beta0))\n",
    "    # loop over the observed data using pyro.sample with the obs keyword argument\n",
    "    for i in range(len(data)):\n",
    "        # observe datapoint i using the bernoulli likelihood\n",
    "        pyro.sample(\"obs_{}\".format(i), dist.Bernoulli(f), obs=data[i])\n",
    "```        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "对于该模型，给定潜变量 `latent_fairness` ，观测样本是条件独立的. To explicitly mark this in Pyro we basically just need to replace the Python builtin `range` with the Pyro construct `plate`:\n",
    "\n",
    "```python\n",
    "# 我们通过 plate 来声明给定潜变量，观测样本之间的条件独立性。\n",
    "def model(data):\n",
    "    # sample f from the beta prior\n",
    "    f = pyro.sample(\"latent_fairness\", dist.Beta(alpha0, beta0))\n",
    "    # loop over the observed data [WE ONLY CHANGE THE NEXT LINE]\n",
    "    for i in pyro.plate(\"data_loop\", len(data)):  \n",
    "        # observe datapoint i using the bernoulli likelihood\n",
    "        pyro.sample(\"obs_{}\".format(i), dist.Bernoulli(f), obs=data[i])\n",
    "```\n",
    "\n",
    "我们看到 `pyro.plate` 与 `range` 非常相似，但有一个关键区别：each invocation of `plate` requires the user to provide **a unique name.** The second argument is an integer just like for `range`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "到目前为止，一切都很好。Pyro现在可以利用给定潜在随机变量下观测值的条件独立性。 But how does this actually work? 基本上，`pyro.plate`是使用上下文管理器(context manager)实现的。 At every execution of the body of the `for` loop we enter a new (conditional) independence context which is then exited at the end of the `for` loop body. Let's be very explicit about this: \n",
    "\n",
    "- because each observed `pyro.sample` statement occurs within a different execution of the body of the `for` loop, Pyro marks each observation as independent\n",
    "- this independence is properly a _conditional_ independence _given_ `latent_fairness` because `latent_fairness` is sampled _outside_ of the context of `data_loop`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在继续之前，让我们提一下使用 sequential `plate` 时要避免的一些陷阱。考虑上述代码片段的以下变体：\n",
    "\n",
    "```python\n",
    "# WARNING 不要这样做!\n",
    "my_reified_list = list(pyro.plate(\"data_loop\", len(data)))\n",
    "for i in my_reified_list:  \n",
    "    pyro.sample(\"obs_{}\".format(i), dist.Bernoulli(f), obs=data[i])\n",
    "```\n",
    "\n",
    "这将无法得到想要的效果, since `list()` will enter and exit the `data_loop` context completely before a single `pyro.sample` statement is called. Similarly, the user needs to take care not to leak mutable computations across the boundary of the context manager, as this may lead to subtle bugs. For example, `pyro.plate`  不适用于时序模型，在该模型中循环的 each iteration 都依赖于 previous iteration; 所以时序模型中应该使用  `range` 或者 `pyro.markov` ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<big>  `plate`  向量化 </big>\n",
    "\n",
    "概念上 vectorized `plate` 和 sequential `plate` 是一样的 except that it is a vectorized operation (as `torch.arange` is to `range`). 因此，它有可能实现大幅提速 compared to the explicit `for` loop that appears with sequential `plate`. Let's see how this looks for our running example. First we need `data` to be in the form of a tensor:\n",
    "\n",
    "```python\n",
    "data = torch.zeros(10)\n",
    "data[0:6] = torch.ones(6)  # 6 heads and 4 tails\n",
    "```\n",
    "\n",
    "Then we have:\n",
    "\n",
    "```python\n",
    "# 向量化 plate 能够帮助加速后续相关计算。\n",
    "with plate('observe_data'):\n",
    "    pyro.sample('obs', dist.Bernoulli(f), obs=data)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们将其与 sequential `plate` 用法进行 point-by-point 比较：\n",
    "- 这两种模式都要求用户指定一个唯一的 name。\n",
    "- 注意这个代码块只引入一个观测随机变量(namely `obs`), since the entire tensor is considered at once. \n",
    "- since there is no need for an iterator in this case, 无需指定 `plate` context 所涉及的张量的长度。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 子采样\n",
    "\n",
    "<big> 对于大数据集，每次训练只能用小部分样本进行训练，也就是 subsampling. </big> \n",
    "\n",
    "现在，我们知道了如何在Pyro中标记条件独立性。这本身就很有用（请参见SVI第III部分中的 [dependency tracking section](svi_part_iii.ipynb)），但是我们也想进行子采样，以便可以对大型数据集进行 SVI 。根据 `model` 和 `guide` 的结构，Pyro支持几种进行子采样的方法。让我们一一讲解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自动子采样 with `plate`\n",
    "\n",
    "Let's look at the simplest case first, in which we get subsampling for free with one or two additional arguments to `plate`:\n",
    "\n",
    "首先让我们看一下最简单的情况，在这种情况下，we get subsampling for free with one or two additional arguments to `plate`:\n",
    "\n",
    "```python\n",
    "for i in pyro.plate(\"data_loop\", len(data), subsample_size=5):\n",
    "    pyro.sample(\"obs_{}\".format(i), dist.Bernoulli(f), obs=data[i])\n",
    "```    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all there is to it: we just use the argument `subsample_size`. Whenever we run `model()` we now only evaluate the log likelihood for `data` 5个随机抽取的样本; 此外，对数似然将自动缩放 by the appropriate factor of $\\tfrac{10}{5} = 2$. 对于向量化 `plate` 如何子采样? 使用方法也完全类似:\n",
    "\n",
    "```python\n",
    "with plate('observe_data', size=10, subsample_size=5) as ind:\n",
    "    pyro.sample('obs', dist.Bernoulli(f), \n",
    "                obs=data.index_select(0, ind))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重要的是，`plate`现在返回一个索引`ind`的张量，在这种情况下，它的长度为5。请注意，除了参数`subsample_size`外，我们还传递了参数`size`，以便`plate`为获得张量 `data` 的完整大小，以便它可以计算正确的缩放因子。就像sequential `plate` 一样，the user is responsible for selecting the correct datapoints using the indices provided by `plate`. \n",
    "\n",
    "最后, 请注意，如果数据在GPU上，则用户必须将 `device` 参数传递给 `plate`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自定义子采样 with `plate`\n",
    "\n",
    "Every time the above `model()` is run `plate` will sample new subsample indices. 由于这种子采样是 stateless，因此可能会导致一些问题：对于足够大的数据集，即使经过大量的迭代，也存在不可忽略的可能性，即从未选择某些数据点。为了避免这种情况，the user can take control of subsampling by making use of the `subsample` argument to  `plate`. See [the docs](http://docs.pyro.ai/en/dev/primitives.html#pyro.plate) for details.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<big> **Subsampling when there are only local random variables**</big>\n",
    "\n",
    "\n",
    "\n",
    "我们考虑具备如下联合概率密度，也就是只有局部随机变量的 `model`:\n",
    "\n",
    "$$ p({\\bf x}, {\\bf z}) = \\prod_{i=1}^N p({\\bf x}_i | {\\bf z}_i) p({\\bf z}_i)  $$\n",
    "\n",
    "For a model with this dependency structure the scale factor introduced by subsampling scales all the terms in the ELBO by the same amount. 例如，vanilla VAE 就是这种情况。 这就解释了为什么对于VAE，user 可以完全控制子采样并将 mini-batches 直接传递给 model and guide; `plate` is still used, but `subsample_size` and `subsample` are not. To see how this looks in detail, see the [VAE tutorial](vae.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<big> **Subsampling when there are both global and local random variables**</big>\n",
    "\n",
    "在掷硬币的例子中 `plate` 只出现在 `model` 而没有出现在 `guide` 中, since the only thing being subsampled was the observations. 让我们看一个更复杂的例子，也就是  `plate` 同时出现在 `model` 而没有出现在 `guide`中. To make things simple let's keep the discussion somewhat abstract and avoid writing a complete model and guide. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "考虑一个具备如下分布的 `model`:\n",
    "\n",
    "$$ p({\\bf x}, {\\bf z}, \\beta) = p(\\beta) \n",
    "\\prod_{i=1}^N p({\\bf x}_i | {\\bf z}_i) p({\\bf z}_i | \\beta)  $$\n",
    "\n",
    "There are $N$ observations $\\{ {\\bf x}_i \\}$ and $N$ local latent random variables \n",
    "$\\{ {\\bf z}_i \\}$. There is also a global latent random variable $\\beta$. 我们的 `gude` 将被分解为\n",
    "\n",
    "$$ q({\\bf z}, \\beta) = q(\\beta) \\prod_{i=1}^N q({\\bf z}_i | \\beta, \\lambda_i)  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we've been explicit about introducing $N$ local variational parameters $\\{\\lambda_i \\}$, while the other variational parameters are left implicit. Both the model and guide have conditional independencies. In particular, \n",
    "- on the model side, given the $\\{ {\\bf z}_i \\}$ the observations $\\{ {\\bf x}_i \\}$ are independent. In addition, given $\\beta$ the latent random variables  $\\{\\bf {z}_i \\}$ are independent. \n",
    "- On the guide side, given the variational parameters $\\{\\lambda_i \\}$ and $\\beta$ the latent random variables  $\\{\\bf {z}_i \\}$ are independent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了在 Pyro 中标记这些条件独立性和进行子采样, 我们需要在 model and guide 中都使用  `plate`. Let's sketch out the basic logic using sequential `plate` (a more complete piece of code would include `pyro.param` statements, etc.). First, the model:\n",
    "\n",
    "```python\n",
    "def model(data):\n",
    "    beta = pyro.sample(\"beta\", ...) # sample the global RV\n",
    "    for i in pyro.plate(\"locals\", len(data)):\n",
    "        z_i = pyro.sample(\"z_{}\".format(i), ...)\n",
    "        # compute the parameter used to define the observation \n",
    "        # likelihood using the local random variable\n",
    "        theta_i = compute_something(z_i) \n",
    "        pyro.sample(\"obs_{}\".format(i), dist.MyDist(theta_i), obs=data[i])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in contrast to our running coin flip example, here we have `pyro.sample` statements both inside and outside of the `plate` loop. Next the guide:\n",
    "\n",
    "```python\n",
    "def guide(data):\n",
    "    beta = pyro.sample(\"beta\", ...) # sample the global RV\n",
    "    for i in pyro.plate(\"locals\", len(data), subsample_size=5):\n",
    "        # sample the local RVs\n",
    "        pyro.sample(\"z_{}\".format(i), ..., lambda_i)\n",
    "```\n",
    "\n",
    "Note that crucially the indices will only be subsampled once in the guide; the Pyro backend makes sure that the same set of indices are used during execution of the model. For this reason `subsample_size` only needs to be specified in the guide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amortization\n",
    "\n",
    "<big> 变分自编码器(VAE)的由来</big>\n",
    "\n",
    "Let's again consider a model with global and local latent random variables and local variational parameters:\n",
    "\n",
    "$$ p({\\bf x}, {\\bf z}, \\beta) = p(\\beta) \n",
    "\\prod_{i=1}^N p({\\bf x}_i | {\\bf z}_i) p({\\bf z}_i | \\beta)  \\qquad \\qquad\n",
    "q({\\bf z}, \\beta) = q(\\beta) \\prod_{i=1}^N q({\\bf z}_i | \\beta, \\lambda_i)  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For small to medium-sized $N$ using local variational parameters like this can be a good approach. If $N$ is large, however, the fact that the space we're doing optimization over grows with $N$ can be a real problem. <big> One way to avoid this nasty growth with the size of the dataset is *amortization*.</big>\n",
    "\n",
    "This works as follows. Instead of introducing local variational parameters, we're going to learn a single parametric function $f(\\cdot)$ and work with a variational distribution that has the form \n",
    "\n",
    "$$q(\\beta) \\prod_{n=1}^N q({\\bf z}_i | f({\\bf x}_i))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function $f(\\cdot)$&mdash;which basically maps a given observation to a set of variational parameters tailored to that datapoint&mdash;will need to be sufficiently rich to capture the posterior accurately, but now we can handle large datasets without having to introduce an obscene number of variational parameters. \n",
    "This approach has other benefits too: for example, during learning $f(\\cdot)$ effectively allows us to share statistical power among different datapoints. Note that this is precisely the approach used in the [VAE](vae.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More about `plate`  \n",
    "\n",
    "<small> Tensor shapes and vectorized `plate` </small> \n",
    "\n",
    "The usage of `pyro.plate` in this tutorial was limited to relatively simple cases. For example, none of the `plate`s were nested inside of other `plate`s. In order to make full use of `plate`, the user must be careful to use Pyro's tensor shape semantics. For a discussion see the [tensor shapes tutorial](tensor_shapes.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考文献\n",
    "\n",
    "[1] `Stochastic Variational Inference`,\n",
    "<br/>&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "Matthew D. Hoffman, David M. Blei, Chong Wang, John Paisley\n",
    "\n",
    "[2] `Auto-Encoding Variational Bayes`,<br/>&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "Diederik P Kingma, Max Welling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
