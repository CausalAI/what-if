

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>变分自编码器 &mdash; Pyro Tutorials 编译 1.3.0 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script type="text/javascript" src="_static/jquery.js"></script>
        <script type="text/javascript" src="_static/underscore.js"></script>
        <script type="text/javascript" src="_static/doctools.js"></script>
        <script type="text/javascript" src="_static/language_data.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/pyro.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="贝叶斯回归- Introduction (Part 1)" href="bayesian_regression.html" />
    <link rel="prev" title="Poutine: Pyro 中使用 Effect Handlers 编程手册" href="effect_handlers.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html">
          

          
            
            <img src="_static/pyro_logo_wide.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                1.3.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Introduction:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="intro_part_i.html">Pyro 模型介绍</a></li>
<li class="toctree-l1"><a class="reference internal" href="intro_part_ii.html">Pyro 推断简介</a></li>
<li class="toctree-l1"><a class="reference internal" href="svi_part_i.html">SVI Part I: Pyro 随机变分推断基础</a></li>
<li class="toctree-l1"><a class="reference internal" href="svi_part_ii.html">SVI Part II: 条件独立, 子采样和 Amortization</a></li>
<li class="toctree-l1"><a class="reference internal" href="svi_part_iii.html">SVI Part III: ELBO Gradient Estimators</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_shapes.html">Tensor shapes in Pyro</a></li>
</ul>
<p class="caption"><span class="caption-text">Advanced:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="enumeration.html">Inference with Discrete Latent Variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="custom_objectives.html">自定义 SVI 目标函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="jit.html">Pyro 模型中使用 PyTorch JIT Compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="minipyro.html">Mini-Pyro</a></li>
<li class="toctree-l1"><a class="reference internal" href="effect_handlers.html">Poutine: Pyro 中使用 Effect Handlers 编程手册</a></li>
</ul>
<p class="caption"><span class="caption-text">Examples:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">变分自编码器</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#VAE-数学简介">VAE 数学简介</a></li>
<li class="toctree-l2"><a class="reference internal" href="#VAE-in-Pyro">VAE in Pyro</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Inference">Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Code-and-Sample-results">Code and Sample results</a></li>
<li class="toctree-l2"><a class="reference internal" href="#参考文献">参考文献</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="bayesian_regression.html">贝叶斯回归- Introduction (Part 1)</a></li>
<li class="toctree-l1"><a class="reference internal" href="bayesian_regression_ii.html">贝叶斯回归-推断算法(Part 2)</a></li>
<li class="toctree-l1"><a class="reference internal" href="dmm.html">深马尔可夫模型</a></li>
<li class="toctree-l1"><a class="reference internal" href="air.html">Attend Infer Repeat</a></li>
<li class="toctree-l1"><a class="reference internal" href="ss-vae.html">半监督 VAE</a></li>
<li class="toctree-l1"><a class="reference internal" href="stable.html">随机波动率的 Levy 稳定分布模型</a></li>
</ul>
<p class="caption"><span class="caption-text">Contributed:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="gmm.html">Gaussian Mixture Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="gp.html">Gaussian Processes</a></li>
<li class="toctree-l1"><a class="reference internal" href="gplvm.html">Gaussian Process Latent Variable Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="bo.html">Bayesian Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="easyguide.html">Writing guides using EasyGuide</a></li>
<li class="toctree-l1"><a class="reference internal" href="forecasting_i.html">Forecasting I: univariate, heavy tailed</a></li>
<li class="toctree-l1"><a class="reference internal" href="forecasting_ii.html">Forecasting II: state space models</a></li>
<li class="toctree-l1"><a class="reference internal" href="forecasting_iii.html">Forecasting III: hierarchical models</a></li>
<li class="toctree-l1"><a class="reference internal" href="tracking_1d.html">Tracking an Unknown Number of Objects</a></li>
<li class="toctree-l1"><a class="reference internal" href="csis.html">Compiled Sequential Importance Sampling</a></li>
<li class="toctree-l1"><a class="reference internal" href="RSA-implicature.html">The Rational Speech Act framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="RSA-hyperbole.html">Understanding Hyperbole using RSA</a></li>
<li class="toctree-l1"><a class="reference internal" href="ekf.html">Kalman Filter</a></li>
<li class="toctree-l1"><a class="reference internal" href="working_memory.html">Designing Adaptive Experiments to Study Working Memory</a></li>
<li class="toctree-l1"><a class="reference internal" href="elections.html">Predicting the outcome of a US presidential election using Bayesian optimal experimental design</a></li>
<li class="toctree-l1"><a class="reference internal" href="dirichlet_process_mixture.html">Dirichlet Process Mixture Models in Pyro</a></li>
<li class="toctree-l1"><a class="reference internal" href="boosting_bbvi.html">Boosting Black Box Variational Inference</a></li>
</ul>
<p class="caption"><span class="caption-text">Code Examples:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="capture_recapture.html">Capture-Recapture Models (CJS Models)</a></li>
<li class="toctree-l1"><a class="reference internal" href="cevae.html">Causal Effect VAE</a></li>
<li class="toctree-l1"><a class="reference internal" href="hmm.html">Hidden Markov Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="lda.html">Latent Dirichlet Allocation</a></li>
<li class="toctree-l1"><a class="reference internal" href="mcmc.html">Markov Chain Monte Carlo</a></li>
<li class="toctree-l1"><a class="reference internal" href="neutra.html">NeuTraReparam</a></li>
<li class="toctree-l1"><a class="reference internal" href="sparse_gamma.html">Sparse Gamma Deep Exponential Family</a></li>
<li class="toctree-l1"><a class="reference internal" href="dkl.html">Deep Kernel Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="einsum.html">Plated Einsum</a></li>
<li class="toctree-l1"><a class="reference internal" href="forecast_simple.html">Multivariate Forecasting</a></li>
<li class="toctree-l1"><a class="reference internal" href="timeseries.html">Gaussian Process Time Series Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="smcfilter.html">Sequential Monte Carlo Filtering</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Pyro Tutorials 编译</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>变分自编码器</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/vae.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container,
div.nbinput.container div.prompt,
div.nbinput.container div.input_area,
div.nbinput.container div[class*=highlight],
div.nbinput.container div[class*=highlight] pre,
div.nboutput.container,
div.nboutput.container div.prompt,
div.nboutput.container div.output_area,
div.nboutput.container div[class*=highlight],
div.nboutput.container div[class*=highlight] pre {
    background: none;
    border: none;
    padding: 0 0;
    margin: 0;
    box-shadow: none;
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    min-width: 5ex;
    padding-top: 0.3rem;
    padding-right: 0.3rem;
    text-align: right;
    flex: 0;
}
@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    background: #f5f5f5;
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 0.3rem;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt a.copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="变分自编码器">
<h1>变分自编码器<a class="headerlink" href="#变分自编码器" title="Permalink to this headline">¶</a></h1>
<div class="section" id="VAE-数学简介">
<h2>VAE 数学简介<a class="headerlink" href="#VAE-数学简介" title="Permalink to this headline">¶</a></h2>
<p>变分自编码器（VAE）可以说是实现深度概率建模的最简单 setup. Note that we’re being careful in our choice of language here. The VAE isn’t a model as such—rather the VAE is a particular setup for doing variational inference for a certain class of models. 模型的类别非常广泛: basically any (unsupervised) density estimator with latent random variables. The basic structure of such a model is simple, almost deceptively so (see Fig. 1).</p>
<center><figure><figcaption><p>Figure 1: the class of deep models we’re interested in.</p>
</figcaption></figure></center><p>Here we’ve depicted the structure of the kind of model we’re interested in as a graphical model. We have <span class="math notranslate nohighlight">\(N\)</span> observed datapoints <span class="math notranslate nohighlight">\(\{ \bf x_i \}\)</span>. Each datapoint is generated by a (local) latent random variable <span class="math notranslate nohighlight">\(\bf z_i\)</span>. There is also a parameter <span class="math notranslate nohighlight">\(\theta\)</span>, which is global in the sense that all the datapoints depend on it (which is why it’s drawn outside the rectangle). Note that since <span class="math notranslate nohighlight">\(\theta\)</span> is a parameter, it’s not something we’re being Bayesian about. Finally,
what’s of particular importance here is that we allow for each <span class="math notranslate nohighlight">\(\bf x_i\)</span> to depend on <span class="math notranslate nohighlight">\(\bf z_i\)</span> in a complex, non-linear way. In practice this dependency will be parameterized by a (deep) neural network with parameters <span class="math notranslate nohighlight">\(\theta\)</span>. It’s this non-linearity that makes inference for this class of models particularly challenging.</p>
<p>Of course this non-linear structure is also one reason why this class of models offers a very flexible approach to modeling complex data. Indeed it’s worth emphasizing that each of the components of the model can be ‘reconfigured’ in a variety of different ways. For example:</p>
<ul class="simple">
<li><p>the neural network in <span class="math notranslate nohighlight">\(p_\theta({\bf x} | {\bf z})\)</span> can be varied in all the usual ways (number of layers, type of non-linearities, number of hidden units, etc.)</p></li>
<li><p>we can choose observation likelihoods that suit the dataset at hand: gaussian, bernoulli, categorical, etc.</p></li>
<li><p>we can choose the number of dimensions in the latent space</p></li>
</ul>
<p>The graphical model representation is a useful way to think about the structure of the model, but it can also be fruitful to look at an explicit factorization of the joint probability density:</p>
<div class="math notranslate nohighlight">
\[p({\bf x}, {\bf z}) = \prod_{i=1}^N p_\theta({\bf x}_i | {\bf z}_i) p({\bf z}_i)\]</div>
<p>The fact that <span class="math notranslate nohighlight">\(p({\bf x}, {\bf z})\)</span> breaks up into a product of terms like this makes it clear what we mean when we call <span class="math notranslate nohighlight">\(\bf z_i\)</span> a local random variable. For any particular <span class="math notranslate nohighlight">\(i\)</span>, only the single datapoint <span class="math notranslate nohighlight">\(\bf x_i\)</span> depends on <span class="math notranslate nohighlight">\(\bf z_i\)</span>. As such the <span class="math notranslate nohighlight">\(\{\bf z_i\}\)</span> describe local structure, i.e. structure that is private to each data point. This factorized structure also means that we can do subsampling during the course of learning. As such this sort of model
is amenable to the large data setting. (For more discussion on this and related topics see <a class="reference internal" href="svi_part_ii.html"><span class="doc">SVI Part II</span></a>.)</p>
<p>That’s all there is to the model. Since the observations depend on the latent random variables in a complicated, non-linear way, we expect the posterior over the latents to have a complex structure. Consequently in order to do inference in this model we need to specify a flexibly family of guides (i.e. variational distributions). Since we want to be able to scale to large datasets, our guide is going to make use of amortization to keep the number of variational parameters under control (see <a class="reference internal" href="svi_part_ii.html"><span class="doc">SVI
Part II</span></a> for a somewhat more general discussion of amortization).</p>
<p>Recall that the job of the guide is to ‘guess’ good values for the latent random variables—good in the sense that they’re true to the model prior <em>and</em> true to the data. If we weren’t making use of amortization, we would introduce variational parameters <span class="math notranslate nohighlight">\(\{ \lambda_i \}\)</span> for <em>each</em> datapoint <span class="math notranslate nohighlight">\(\bf x_i\)</span>. These variational parameters would represent our belief about ‘good’ values of <span class="math notranslate nohighlight">\(\bf z_i\)</span>; for example, they could encode the mean and variance of a gaussian distribution in
<span class="math notranslate nohighlight">\({\bf z}_i\)</span> space. Amortization means that, rather than introducing variational parameters <span class="math notranslate nohighlight">\(\{ \lambda_i \}\)</span>, we instead learn a <em>function</em> that maps each <span class="math notranslate nohighlight">\(\bf x_i\)</span> to an appropriate <span class="math notranslate nohighlight">\(\lambda_i\)</span>. Since we need this function to be flexible, we parameterize it as a neural network. We thus end up with a parameterized family of distributions over the latent <span class="math notranslate nohighlight">\(\bf z\)</span> space that can be instantiated for all <span class="math notranslate nohighlight">\(N\)</span> datapoint <span class="math notranslate nohighlight">\({\bf x}_i\)</span> (see Fig. 2).</p>
<center><figure><figcaption><p>Figure 2: a graphical representation of the guide.</p>
</figcaption></figure></center><p>Note that the guide <span class="math notranslate nohighlight">\(q_{\phi}({\bf z} | {\bf x})\)</span> is parameterized by a global parameter <span class="math notranslate nohighlight">\(\phi\)</span> shared by all the datapoints. The goal of inference will be to find ‘good’ values for <span class="math notranslate nohighlight">\(\theta\)</span> and <span class="math notranslate nohighlight">\(\phi\)</span> so that two conditions are satisfied:</p>
<ul class="simple">
<li><p>the log evidence <span class="math notranslate nohighlight">\(\log p_\theta({\bf x})\)</span> is large. this means our model is a good fit to the data</p></li>
<li><p>the guide <span class="math notranslate nohighlight">\(q_{\phi}({\bf z} | {\bf x})\)</span> provides a good approximation to the posterior</p></li>
</ul>
<p>(For an introduction to stochastic variational inference see <a class="reference internal" href="svi_part_i.html"><span class="doc">SVI Part I</span></a>.)</p>
<p>At this point we can zoom out and consider the high level structure of our setup. For concreteness, let’s suppose the <span class="math notranslate nohighlight">\(\{ \bf x_i \}\)</span> are images so that the model is a generative model of images. Once we’ve learned a good value of <span class="math notranslate nohighlight">\(\theta\)</span> we can generate images from the model as follows:</p>
<ul class="simple">
<li><p>sample <span class="math notranslate nohighlight">\(\bf z\)</span> according to the prior <span class="math notranslate nohighlight">\(p({\bf z})\)</span></p></li>
<li><p>sample <span class="math notranslate nohighlight">\(\bf x\)</span> according to the likelihood <span class="math notranslate nohighlight">\(p_\theta({\bf x}|{\bf z})\)</span></p></li>
</ul>
<p>Each image is being represented by a latent code <span class="math notranslate nohighlight">\(\bf z\)</span> and that code gets mapped to images using the likelihood, which depends on the <span class="math notranslate nohighlight">\(\theta\)</span> we’ve learned. This is why the likelihood is often called the decoder in this context: its job is to decode <span class="math notranslate nohighlight">\(\bf z\)</span> into <span class="math notranslate nohighlight">\(\bf x\)</span>. Note that since this is a probabilistic model, there is uncertainty about the <span class="math notranslate nohighlight">\(\bf z\)</span> that encodes a given datapoint <span class="math notranslate nohighlight">\(\bf x\)</span>.</p>
<p>Once we’ve learned good values for <span class="math notranslate nohighlight">\(\theta\)</span> and <span class="math notranslate nohighlight">\(\phi\)</span> we can also go through the following exercise.</p>
<ul class="simple">
<li><p>we start with a given image <span class="math notranslate nohighlight">\(\bf x\)</span></p></li>
<li><p>using our guide we encode it as <span class="math notranslate nohighlight">\(\bf z\)</span></p></li>
<li><p>using the model likelihood we decode <span class="math notranslate nohighlight">\(\bf z\)</span> and get a reconstructed image <span class="math notranslate nohighlight">\({\bf x}_\rm{reco}\)</span></p></li>
</ul>
<p>If we’ve learned good values for <span class="math notranslate nohighlight">\(\theta\)</span> and <span class="math notranslate nohighlight">\(\phi\)</span>, <span class="math notranslate nohighlight">\(\bf x\)</span> and <span class="math notranslate nohighlight">\({\bf x}_\rm{reco}\)</span> should be similar. This should clarify how the word autoencoder ended up being used to describe this setup: the model is the decoder and the guide is the encoder. Together, they can be thought of as an autoencoder.</p>
</div>
<div class="section" id="VAE-in-Pyro">
<h2>VAE in Pyro<a class="headerlink" href="#VAE-in-Pyro" title="Permalink to this headline">¶</a></h2>
<p>Let’s see how we implement a VAE in Pyro. The dataset we’re going to model is MNIST, a collection of images of handwritten digits. Since this is a popular benchmark dataset, we can make use of PyTorch’s convenient data loader functionalities to reduce the amount of boilerplate code we need to write:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">os</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torchvision.datasets</span> <span class="k">as</span> <span class="nn">dset</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="k">as</span> <span class="nn">transforms</span>

<span class="kn">import</span> <span class="nn">pyro</span>
<span class="kn">import</span> <span class="nn">pyro.distributions</span> <span class="k">as</span> <span class="nn">dist</span>
<span class="kn">import</span> <span class="nn">pyro.contrib.examples.util</span>  <span class="c1"># patches torchvision</span>
<span class="kn">from</span> <span class="nn">pyro.infer</span> <span class="k">import</span> <span class="n">SVI</span><span class="p">,</span> <span class="n">Trace_ELBO</span>
<span class="kn">from</span> <span class="nn">pyro.optim</span> <span class="k">import</span> <span class="n">Adam</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">assert</span> <span class="n">pyro</span><span class="o">.</span><span class="n">__version__</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;1.3.0&#39;</span><span class="p">)</span>
<span class="n">pyro</span><span class="o">.</span><span class="n">enable_validation</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">pyro</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">enable_validation</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="n">pyro</span><span class="o">.</span><span class="n">set_rng_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="c1"># Enable smoke test - run the notebook cells on CI.</span>
<span class="n">smoke_test</span> <span class="o">=</span> <span class="s1">&#39;CI&#39;</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># for loading and batching MNIST dataset</span>
<span class="k">def</span> <span class="nf">setup_data_loaders</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">use_cuda</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="n">root</span> <span class="o">=</span> <span class="s1">&#39;./data&#39;</span>
    <span class="n">download</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">trans</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">()</span>
    <span class="n">train_set</span> <span class="o">=</span> <span class="n">dset</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="n">root</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">trans</span><span class="p">,</span>
                           <span class="n">download</span><span class="o">=</span><span class="n">download</span><span class="p">)</span>
    <span class="n">test_set</span> <span class="o">=</span> <span class="n">dset</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="n">root</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">trans</span><span class="p">)</span>

    <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;num_workers&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;pin_memory&#39;</span><span class="p">:</span> <span class="n">use_cuda</span><span class="p">}</span>
    <span class="n">train_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">train_set</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="n">test_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">test_set</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">test_loader</span>
</pre></div>
</div>
</div>
<p>The main thing to draw attention to here is that we use <code class="docutils literal notranslate"><span class="pre">transforms.ToTensor()</span></code> to normalize the pixel intensities to the range <span class="math notranslate nohighlight">\([0.0, 1.0]\)</span>.</p>
<p>Next we define a PyTorch module that encapsulates our decoder network:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">class</span> <span class="nc">Decoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># setup the two linear transformations used</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc21</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="mi">784</span><span class="p">)</span>
        <span class="c1"># setup the non-linearities</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">softplus</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softplus</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">z</span><span class="p">):</span>
        <span class="c1"># define the forward computation on the latent z</span>
        <span class="c1"># first compute the hidden units</span>
        <span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softplus</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">z</span><span class="p">))</span>
        <span class="c1"># return the parameter for the output Bernoulli</span>
        <span class="c1"># each is of size batch_size x 784</span>
        <span class="n">loc_img</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc21</span><span class="p">(</span><span class="n">hidden</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">loc_img</span>
</pre></div>
</div>
</div>
<p>Given a latent code <span class="math notranslate nohighlight">\(z\)</span>, the forward call of <code class="docutils literal notranslate"><span class="pre">Decoder</span></code> returns the parameters for a Bernoulli distribution in image space. Since each image is of size <span class="math notranslate nohighlight">\(28\times28=784\)</span>, <code class="docutils literal notranslate"><span class="pre">loc_img</span></code> is of size <code class="docutils literal notranslate"><span class="pre">batch_size</span></code> x 784.</p>
<p>Next we define a PyTorch module that encapsulates our encoder network:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">class</span> <span class="nc">Encoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># setup the three linear transformations used</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc21</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc22</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">)</span>
        <span class="c1"># setup the non-linearities</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">softplus</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softplus</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># define the forward computation on the image x</span>
        <span class="c1"># first shape the mini-batch to have pixels in the rightmost dimension</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">)</span>
        <span class="c1"># then compute the hidden units</span>
        <span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softplus</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="c1"># then return a mean vector and a (positive) square root covariance</span>
        <span class="c1"># each of size batch_size x z_dim</span>
        <span class="n">z_loc</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc21</span><span class="p">(</span><span class="n">hidden</span><span class="p">)</span>
        <span class="n">z_scale</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc22</span><span class="p">(</span><span class="n">hidden</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">z_loc</span><span class="p">,</span> <span class="n">z_scale</span>
</pre></div>
</div>
</div>
<p>Given an image <span class="math notranslate nohighlight">\(\bf x\)</span> the forward call of <code class="docutils literal notranslate"><span class="pre">Encoder</span></code> returns a mean and covariance that together parameterize a (diagonal) Gaussian distribution in latent space.</p>
<p>With our encoder and decoder networks in hand, we can now write down the stochastic functions that represent our model and guide. First the model:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># define the model p(x|z)p(z)</span>
<span class="k">def</span> <span class="nf">model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="c1"># register PyTorch module `decoder` with Pyro</span>
    <span class="n">pyro</span><span class="o">.</span><span class="n">module</span><span class="p">(</span><span class="s2">&quot;decoder&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">pyro</span><span class="o">.</span><span class="n">plate</span><span class="p">(</span><span class="s2">&quot;data&quot;</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="c1"># setup hyperparameters for prior p(z)</span>
        <span class="n">z_loc</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">new_zeros</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">((</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">z_dim</span><span class="p">)))</span>
        <span class="n">z_scale</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">new_ones</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">((</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">z_dim</span><span class="p">)))</span>
        <span class="c1"># sample from prior (value will be sampled by guide when computing the ELBO)</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">pyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s2">&quot;latent&quot;</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">z_loc</span><span class="p">,</span> <span class="n">z_scale</span><span class="p">)</span><span class="o">.</span><span class="n">to_event</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
        <span class="c1"># decode the latent code z</span>
        <span class="n">loc_img</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="c1"># score against actual images</span>
        <span class="n">pyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s2">&quot;obs&quot;</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Bernoulli</span><span class="p">(</span><span class="n">loc_img</span><span class="p">)</span><span class="o">.</span><span class="n">to_event</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">obs</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">))</span>
</pre></div>
</div>
</div>
<p>Note that <code class="docutils literal notranslate"><span class="pre">model()</span></code> is a callable that takes in a mini-batch of images <code class="docutils literal notranslate"><span class="pre">x</span></code> as input. This is a <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code> of size <code class="docutils literal notranslate"><span class="pre">batch_size</span></code> x 784.</p>
<p>The first thing we do inside of <code class="docutils literal notranslate"><span class="pre">model()</span></code> is register the (previously instantiated) decoder module with Pyro. Note that we give it an appropriate (and unique) name. This call to <code class="docutils literal notranslate"><span class="pre">pyro.module</span></code> lets Pyro know about all the parameters inside of the decoder network.</p>
<p>Next we setup the hyperparameters for our prior, which is just a unit normal gaussian distribution. Note that: - we specifically designate independence amongst the data in our mini-batch (i.e. the leftmost dimension) via <code class="docutils literal notranslate"><span class="pre">pyro.plate</span></code>. Also, note the use of <code class="docutils literal notranslate"><span class="pre">.to_event(1)</span></code> when sampling from the latent <code class="docutils literal notranslate"><span class="pre">z</span></code> - this ensures that instead of treating our sample as being generated from a univariate normal with <code class="docutils literal notranslate"><span class="pre">batch_size</span> <span class="pre">=</span> <span class="pre">z_dim</span></code>, we treat them as being generated from a multivariate normal
distribution with diagonal covariance. As such, the log probabilities along each dimension is summed out when we evaluate <code class="docutils literal notranslate"><span class="pre">.log_prob</span></code> for a “latent” sample. Refer to the <a class="reference internal" href="tensor_shapes.html"><span class="doc">Tensor Shapes</span></a> tutorial for more details. - since we’re processing an entire mini-batch of images, we need the leftmost dimension of <code class="docutils literal notranslate"><span class="pre">z_loc</span></code> and <code class="docutils literal notranslate"><span class="pre">z_scale</span></code> to equal the mini-batch size - in case we’re on GPU, we use <code class="docutils literal notranslate"><span class="pre">new_zeros</span></code> and <code class="docutils literal notranslate"><span class="pre">new_ones</span></code> to ensure that newly created tensors are on the same
GPU device.</p>
<p>Next we sample the latent <code class="docutils literal notranslate"><span class="pre">z</span></code> from the prior, making sure to give the random variable a unique Pyro name <code class="docutils literal notranslate"><span class="pre">'latent'</span></code>. Then we pass <code class="docutils literal notranslate"><span class="pre">z</span></code> through the decoder network, which returns <code class="docutils literal notranslate"><span class="pre">loc_img</span></code>. We then score the observed images in the mini-batch <code class="docutils literal notranslate"><span class="pre">x</span></code> against the Bernoulli likelihood parametrized by <code class="docutils literal notranslate"><span class="pre">loc_img</span></code>. Note that we flatten <code class="docutils literal notranslate"><span class="pre">x</span></code> so that all the pixels are in the rightmost dimension.</p>
<p>That’s all there is to it! Note how closely the flow of Pyro primitives in <code class="docutils literal notranslate"><span class="pre">model</span></code> follows the generative story of our model, e.g. as encapsulated by Figure 1. Now we move on to the guide:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># define the guide (i.e. variational distribution) q(z|x)</span>
<span class="k">def</span> <span class="nf">guide</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="c1"># register PyTorch module `encoder` with Pyro</span>
    <span class="n">pyro</span><span class="o">.</span><span class="n">module</span><span class="p">(</span><span class="s2">&quot;encoder&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">pyro</span><span class="o">.</span><span class="n">plate</span><span class="p">(</span><span class="s2">&quot;data&quot;</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="c1"># use the encoder to get the parameters used to define q(z|x)</span>
        <span class="n">z_loc</span><span class="p">,</span> <span class="n">z_scale</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># sample the latent code z</span>
        <span class="n">pyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s2">&quot;latent&quot;</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">z_loc</span><span class="p">,</span> <span class="n">z_scale</span><span class="p">)</span><span class="o">.</span><span class="n">to_event</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
</div>
<p>Just like in the model, we first register the PyTorch module we’re using (namely <code class="docutils literal notranslate"><span class="pre">encoder</span></code>) with Pyro. We take the mini-batch of images <code class="docutils literal notranslate"><span class="pre">x</span></code> and pass it through the encoder. Then using the parameters output by the encoder network we use the normal distribution to sample a value of the latent for each image in the mini-batch. Crucially, we use the same name for the latent random variable as we did in the model: <code class="docutils literal notranslate"><span class="pre">'latent'</span></code>. Also, note the use of <code class="docutils literal notranslate"><span class="pre">pyro.plate</span></code> to designate independence of the
mini-batch dimension, and <code class="docutils literal notranslate"><span class="pre">.to_event(1)</span></code> to enforce dependence on <code class="docutils literal notranslate"><span class="pre">z_dims</span></code>, exactly as we did in the model.</p>
<p>Now that we’ve defined the full model and guide we can move on to inference. But before we do so let’s see how we package the model and guide in a PyTorch module:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">class</span> <span class="nc">VAE</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="c1"># by default our latent space is 50-dimensional</span>
    <span class="c1"># and we use 400 hidden units</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">z_dim</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">400</span><span class="p">,</span> <span class="n">use_cuda</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># create the encoder and decoder networks</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">Encoder</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">Decoder</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">use_cuda</span><span class="p">:</span>
            <span class="c1"># calling cuda() here will put all the parameters of</span>
            <span class="c1"># the encoder and decoder networks into gpu memory</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_cuda</span> <span class="o">=</span> <span class="n">use_cuda</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">z_dim</span> <span class="o">=</span> <span class="n">z_dim</span>

    <span class="c1"># define the model p(x|z)p(z)</span>
    <span class="k">def</span> <span class="nf">model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># register PyTorch module `decoder` with Pyro</span>
        <span class="n">pyro</span><span class="o">.</span><span class="n">module</span><span class="p">(</span><span class="s2">&quot;decoder&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">pyro</span><span class="o">.</span><span class="n">plate</span><span class="p">(</span><span class="s2">&quot;data&quot;</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="c1"># setup hyperparameters for prior p(z)</span>
            <span class="n">z_loc</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">new_zeros</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">((</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">z_dim</span><span class="p">)))</span>
            <span class="n">z_scale</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">new_ones</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">((</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">z_dim</span><span class="p">)))</span>
            <span class="c1"># sample from prior (value will be sampled by guide when computing the ELBO)</span>
            <span class="n">z</span> <span class="o">=</span> <span class="n">pyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s2">&quot;latent&quot;</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">z_loc</span><span class="p">,</span> <span class="n">z_scale</span><span class="p">)</span><span class="o">.</span><span class="n">to_event</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
            <span class="c1"># decode the latent code z</span>
            <span class="n">loc_img</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
            <span class="c1"># score against actual images</span>
            <span class="n">pyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s2">&quot;obs&quot;</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Bernoulli</span><span class="p">(</span><span class="n">loc_img</span><span class="p">)</span><span class="o">.</span><span class="n">to_event</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">obs</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">))</span>

    <span class="c1"># define the guide (i.e. variational distribution) q(z|x)</span>
    <span class="k">def</span> <span class="nf">guide</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># register PyTorch module `encoder` with Pyro</span>
        <span class="n">pyro</span><span class="o">.</span><span class="n">module</span><span class="p">(</span><span class="s2">&quot;encoder&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">pyro</span><span class="o">.</span><span class="n">plate</span><span class="p">(</span><span class="s2">&quot;data&quot;</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="c1"># use the encoder to get the parameters used to define q(z|x)</span>
            <span class="n">z_loc</span><span class="p">,</span> <span class="n">z_scale</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="c1"># sample the latent code z</span>
            <span class="n">pyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s2">&quot;latent&quot;</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">z_loc</span><span class="p">,</span> <span class="n">z_scale</span><span class="p">)</span><span class="o">.</span><span class="n">to_event</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>

    <span class="c1"># define a helper function for reconstructing images</span>
    <span class="k">def</span> <span class="nf">reconstruct_img</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># encode image x</span>
        <span class="n">z_loc</span><span class="p">,</span> <span class="n">z_scale</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># sample in latent space</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">z_loc</span><span class="p">,</span> <span class="n">z_scale</span><span class="p">)</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
        <span class="c1"># decode the image (note we don&#39;t sample in image space)</span>
        <span class="n">loc_img</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loc_img</span>
</pre></div>
</div>
</div>
<p>The point we’d like to make here is that the two <code class="docutils literal notranslate"><span class="pre">Module</span></code>s <code class="docutils literal notranslate"><span class="pre">encoder</span></code> and <code class="docutils literal notranslate"><span class="pre">decoder</span></code> are attributes of <code class="docutils literal notranslate"><span class="pre">VAE</span></code> (which itself inherits from <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code>). This has the consequence they are both automatically registered as belonging to the <code class="docutils literal notranslate"><span class="pre">VAE</span></code> module. So, for example, when we call <code class="docutils literal notranslate"><span class="pre">parameters()</span></code> on an instance of <code class="docutils literal notranslate"><span class="pre">VAE</span></code>, PyTorch will know to return all the relevant parameters. It also means that if we’re running on a GPU, the call to <code class="docutils literal notranslate"><span class="pre">cuda()</span></code> will move all the parameters of all the
(sub)modules into GPU memory.</p>
</div>
<div class="section" id="Inference">
<h2>Inference<a class="headerlink" href="#Inference" title="Permalink to this headline">¶</a></h2>
<p>We’re now ready for inference. Refer to the full code in the next section.</p>
<p>First we instantiate an instance of the <code class="docutils literal notranslate"><span class="pre">VAE</span></code> module.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">vae</span> <span class="o">=</span> <span class="n">VAE</span><span class="p">()</span>
</pre></div>
</div>
</div>
<p>Then we setup an instance of the Adam optimizer.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">({</span><span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="mf">1.0e-3</span><span class="p">})</span>
</pre></div>
</div>
</div>
<p>Then we setup our inference algorithm, which is going to learn good parameters for the model and guide by maximizing the ELBO:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">svi</span> <span class="o">=</span> <span class="n">SVI</span><span class="p">(</span><span class="n">vae</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">vae</span><span class="o">.</span><span class="n">guide</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="n">Trace_ELBO</span><span class="p">())</span>
</pre></div>
</div>
</div>
<p>That’s all there is to it. Now we just have to define our training loop:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">svi</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">use_cuda</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># initialize loss accumulator</span>
    <span class="n">epoch_loss</span> <span class="o">=</span> <span class="mf">0.</span>
    <span class="c1"># do a training epoch over each mini-batch x returned</span>
    <span class="c1"># by the data loader</span>
    <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
        <span class="c1"># if on GPU put mini-batch into CUDA memory</span>
        <span class="k">if</span> <span class="n">use_cuda</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
        <span class="c1"># do ELBO gradient and accumulate loss</span>
        <span class="n">epoch_loss</span> <span class="o">+=</span> <span class="n">svi</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="c1"># return epoch loss</span>
    <span class="n">normalizer_train</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span>
    <span class="n">total_epoch_loss_train</span> <span class="o">=</span> <span class="n">epoch_loss</span> <span class="o">/</span> <span class="n">normalizer_train</span>
    <span class="k">return</span> <span class="n">total_epoch_loss_train</span>
</pre></div>
</div>
</div>
<p>Note that all the mini-batch logic is handled by the data loader. The meat of the training loop is <code class="docutils literal notranslate"><span class="pre">svi.step(x)</span></code>. There are two things we should draw attention to here:</p>
<ul class="simple">
<li><p>any arguments to <code class="docutils literal notranslate"><span class="pre">step</span></code> are passed to the model and the guide. consequently <code class="docutils literal notranslate"><span class="pre">model</span></code> and <code class="docutils literal notranslate"><span class="pre">guide</span></code> need to have the same call signature</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">step</span></code> returns a noisy estimate of the loss (i.e. minus the ELBO). this estimate is not normalized in any way, so e.g. it scales with the size of the mini-batch</p></li>
</ul>
<p>The logic for adding evaluation logic is analogous:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="n">svi</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">,</span> <span class="n">use_cuda</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># initialize loss accumulator</span>
    <span class="n">test_loss</span> <span class="o">=</span> <span class="mf">0.</span>
    <span class="c1"># compute the loss over the entire test set</span>
    <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">test_loader</span><span class="p">:</span>
        <span class="c1"># if on GPU put mini-batch into CUDA memory</span>
        <span class="k">if</span> <span class="n">use_cuda</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
        <span class="c1"># compute ELBO estimate and accumulate loss</span>
        <span class="n">test_loss</span> <span class="o">+=</span> <span class="n">svi</span><span class="o">.</span><span class="n">evaluate_loss</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">normalizer_test</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_loader</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span>
    <span class="n">total_epoch_loss_test</span> <span class="o">=</span> <span class="n">test_loss</span> <span class="o">/</span> <span class="n">normalizer_test</span>
    <span class="k">return</span> <span class="n">total_epoch_loss_test</span>
</pre></div>
</div>
</div>
<p>Basically the only change we need to make is that we call evaluate_loss instead of step. This function will compute an estimate of the ELBO but won’t take any gradient steps.</p>
<p>The final piece of code we’d like to highlight is the helper method <code class="docutils literal notranslate"><span class="pre">reconstruct_img</span></code> in the VAE class: This is just the image reconstruction experiment we described in the introduction translated into code. We take an image and pass it through the encoder. Then we sample in latent space using the gaussian distribution provided by the encoder. Finally we decode the latent code into an image: we return the mean vector <code class="docutils literal notranslate"><span class="pre">loc_img</span></code> instead of sampling with it. Note that since the <code class="docutils literal notranslate"><span class="pre">sample()</span></code>
statement is stochastic, we’ll get different draws of z every time we run the reconstruct_img function. If we’ve learned a good model and guide—in particular if we’ve learned a good latent representation—this plurality of z samples will correspond to different styles of digit writing, and the reconstructed images should exhibit an interesting variety of different styles.</p>
</div>
<div class="section" id="Code-and-Sample-results">
<h2>Code and Sample results<a class="headerlink" href="#Code-and-Sample-results" title="Permalink to this headline">¶</a></h2>
<p>Training corresponds to maximizing the evidence lower bound (ELBO) over the training dataset. We train for 100 iterations and evaluate the ELBO for the test dataset, see Figure 3.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Run options</span>
<span class="n">LEARNING_RATE</span> <span class="o">=</span> <span class="mf">1.0e-3</span>
<span class="n">USE_CUDA</span> <span class="o">=</span> <span class="kc">False</span>

<span class="c1"># Run only for a single iteration for testing</span>
<span class="n">NUM_EPOCHS</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">smoke_test</span> <span class="k">else</span> <span class="mi">100</span>
<span class="n">TEST_FREQUENCY</span> <span class="o">=</span> <span class="mi">5</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">train_loader</span><span class="p">,</span> <span class="n">test_loader</span> <span class="o">=</span> <span class="n">setup_data_loaders</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">use_cuda</span><span class="o">=</span><span class="n">USE_CUDA</span><span class="p">)</span>

<span class="c1"># clear param store</span>
<span class="n">pyro</span><span class="o">.</span><span class="n">clear_param_store</span><span class="p">()</span>

<span class="c1"># setup the VAE</span>
<span class="n">vae</span> <span class="o">=</span> <span class="n">VAE</span><span class="p">(</span><span class="n">use_cuda</span><span class="o">=</span><span class="n">USE_CUDA</span><span class="p">)</span>

<span class="c1"># setup the optimizer</span>
<span class="n">adam_args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="n">LEARNING_RATE</span><span class="p">}</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">adam_args</span><span class="p">)</span>

<span class="c1"># setup the inference algorithm</span>
<span class="n">svi</span> <span class="o">=</span> <span class="n">SVI</span><span class="p">(</span><span class="n">vae</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">vae</span><span class="o">.</span><span class="n">guide</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="n">Trace_ELBO</span><span class="p">())</span>

<span class="n">train_elbo</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">test_elbo</span> <span class="o">=</span> <span class="p">[]</span>
<span class="c1"># training loop</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">NUM_EPOCHS</span><span class="p">):</span>
    <span class="n">total_epoch_loss_train</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">svi</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">use_cuda</span><span class="o">=</span><span class="n">USE_CUDA</span><span class="p">)</span>
    <span class="n">train_elbo</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="o">-</span><span class="n">total_epoch_loss_train</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;[epoch </span><span class="si">%03d</span><span class="s2">]  average training loss: </span><span class="si">%.4f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">total_epoch_loss_train</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="n">TEST_FREQUENCY</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># report test diagnostics</span>
        <span class="n">total_epoch_loss_test</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">svi</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">,</span> <span class="n">use_cuda</span><span class="o">=</span><span class="n">USE_CUDA</span><span class="p">)</span>
        <span class="n">test_elbo</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="o">-</span><span class="n">total_epoch_loss_test</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;[epoch </span><span class="si">%03d</span><span class="s2">] average test loss: </span><span class="si">%.4f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">total_epoch_loss_test</span><span class="p">))</span>
</pre></div>
</div>
</div>
<center><figure><figcaption><p>Figure 3: How the test ELBO evolves over the course of training.</p>
</figcaption></figure></center><p>Next we show a set of randomly sampled images from the model. These are generated by drawing random samples of <code class="docutils literal notranslate"><span class="pre">z</span></code> and generating an image for each one, see Figure 4.</p>
<center><figure><table><tr><td></td><td></td></tr></table><figcaption><p>Figure 4: Samples from generative model.</p>
</figcaption></figure></center><p>We also study the 50-dimensional latent space of the entire test dataset by encoding all MNIST images and embedding their means into a 2-dimensional T-SNE space. We then color each embedded image by its class. The resulting Figure 5 shows separation by class with variance within each class-cluster.</p>
<center><figure><figcaption><p>Figure 5: T-SNE Embedding of the latent z. The colors correspond to different classes of digits.</p>
</figcaption></figure></center><p>See the full code on <a class="reference external" href="https://github.com/pyro-ppl/pyro/blob/dev/examples/vae/vae.py">Github</a>.</p>
</div>
<div class="section" id="参考文献">
<h2>参考文献<a class="headerlink" href="#参考文献" title="Permalink to this headline">¶</a></h2>
<p>[1] <code class="docutils literal notranslate"><span class="pre">Auto-Encoding</span> <span class="pre">Variational</span> <span class="pre">Bayes</span></code>,     Diederik P Kingma, Max Welling</p>
<p>[2] <code class="docutils literal notranslate"><span class="pre">Stochastic</span> <span class="pre">Backpropagation</span> <span class="pre">and</span> <span class="pre">Approximate</span> <span class="pre">Inference</span> <span class="pre">in</span> <span class="pre">Deep</span> <span class="pre">Generative</span> <span class="pre">Models</span></code>,      Danilo Jimenez Rezende, Shakir Mohamed, Daan Wierstra</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span>
</pre></div>
</div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="bayesian_regression.html" class="btn btn-neutral float-right" title="贝叶斯回归- Introduction (Part 1)" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="effect_handlers.html" class="btn btn-neutral float-left" title="Poutine: Pyro 中使用 Effect Handlers 编程手册" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright Uber Technologies, Inc; 编译 by Heyang Gong

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>