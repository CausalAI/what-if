

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Dirichlet Process Mixture Models in Pyro &mdash; Pyro Tutorials 编译 1.3.0 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script type="text/javascript" src="_static/jquery.js"></script>
        <script type="text/javascript" src="_static/underscore.js"></script>
        <script type="text/javascript" src="_static/doctools.js"></script>
        <script type="text/javascript" src="_static/language_data.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/pyro.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Boosting Black Box Variational Inference" href="boosting_bbvi.html" />
    <link rel="prev" title="Predicting the outcome of a US presidential election using Bayesian optimal experimental design" href="elections.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html">
          

          
            
            <img src="_static/pyro_logo_wide.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                1.3.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Introduction:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="intro_part_i.html">Pyro 模型介绍</a></li>
<li class="toctree-l1"><a class="reference internal" href="intro_part_ii.html">Pyro 推断简介</a></li>
<li class="toctree-l1"><a class="reference internal" href="svi_part_i.html">SVI Part I: Pyro 随机变分推断基础</a></li>
<li class="toctree-l1"><a class="reference internal" href="svi_part_ii.html">SVI Part II: 条件独立, 子采样和 Amortization</a></li>
<li class="toctree-l1"><a class="reference internal" href="svi_part_iii.html">SVI Part III: ELBO Gradient Estimators</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_shapes.html">Tensor shapes in Pyro</a></li>
</ul>
<p class="caption"><span class="caption-text">Advanced:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="enumeration.html">Inference with Discrete Latent Variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="custom_objectives.html">自定义 SVI 目标函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="jit.html">Pyro 模型中使用 PyTorch JIT Compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="minipyro.html">Mini-Pyro</a></li>
<li class="toctree-l1"><a class="reference internal" href="effect_handlers.html">Poutine: Pyro 中使用 Effect Handlers 编程手册</a></li>
</ul>
<p class="caption"><span class="caption-text">Examples:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="vae.html">变分自编码器</a></li>
<li class="toctree-l1"><a class="reference internal" href="bayesian_regression.html">贝叶斯回归- Introduction (Part 1)</a></li>
<li class="toctree-l1"><a class="reference internal" href="bayesian_regression_ii.html">贝叶斯回归-推断算法(Part 2)</a></li>
<li class="toctree-l1"><a class="reference internal" href="dmm.html">深马尔可夫模型</a></li>
<li class="toctree-l1"><a class="reference internal" href="air.html">Attend Infer Repeat</a></li>
<li class="toctree-l1"><a class="reference internal" href="ss-vae.html">半监督 VAE</a></li>
<li class="toctree-l1"><a class="reference internal" href="stable.html">随机波动率的 Levy 稳定分布模型</a></li>
</ul>
<p class="caption"><span class="caption-text">Contributed:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="gmm.html">Gaussian Mixture Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="gp.html">Gaussian Processes</a></li>
<li class="toctree-l1"><a class="reference internal" href="gplvm.html">Gaussian Process Latent Variable Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="bo.html">Bayesian Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="easyguide.html">Writing guides using EasyGuide</a></li>
<li class="toctree-l1"><a class="reference internal" href="forecasting_i.html">Forecasting I: univariate, heavy tailed</a></li>
<li class="toctree-l1"><a class="reference internal" href="forecasting_ii.html">Forecasting II: state space models</a></li>
<li class="toctree-l1"><a class="reference internal" href="forecasting_iii.html">Forecasting III: hierarchical models</a></li>
<li class="toctree-l1"><a class="reference internal" href="tracking_1d.html">Tracking an Unknown Number of Objects</a></li>
<li class="toctree-l1"><a class="reference internal" href="csis.html">Compiled Sequential Importance Sampling</a></li>
<li class="toctree-l1"><a class="reference internal" href="RSA-implicature.html">The Rational Speech Act framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="RSA-hyperbole.html">Understanding Hyperbole using RSA</a></li>
<li class="toctree-l1"><a class="reference internal" href="ekf.html">Kalman Filter</a></li>
<li class="toctree-l1"><a class="reference internal" href="working_memory.html">Designing Adaptive Experiments to Study Working Memory</a></li>
<li class="toctree-l1"><a class="reference internal" href="elections.html">Predicting the outcome of a US presidential election using Bayesian optimal experimental design</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Dirichlet Process Mixture Models in Pyro</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#What-are-Bayesian-nonparametric-models?">What are Bayesian nonparametric models?</a></li>
<li class="toctree-l2"><a class="reference internal" href="#The-Dirichlet-Process-(Ferguson,-1973)">The Dirichlet Process (Ferguson, 1973)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#The-Chinese-Restaurant-Process-(Aldous,-1985)">The Chinese Restaurant Process (Aldous, 1985)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#The-Stick-Breaking-Method-(Sethuraman,-1994)">The Stick-Breaking Method (Sethuraman, 1994)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Inference"><strong>Inference</strong></a><ul>
<li class="toctree-l3"><a class="reference internal" href="#Synthetic-Mixture-of-Gaussians">Synthetic Mixture of Gaussians</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Dirichlet-Mixture-Model-for-Long-Term-Solar-Observations">Dirichlet Mixture Model for Long Term Solar Observations</a></li>
<li class="toctree-l3"><a class="reference internal" href="#ELBO-Behavior">ELBO Behavior</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#Criticism"><strong>Criticism</strong></a><ul>
<li class="toctree-l3"><a class="reference internal" href="#Long-Term-Sunspot-Model">Long-Term Sunspot Model</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#References">References</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="boosting_bbvi.html">Boosting Black Box Variational Inference</a></li>
</ul>
<p class="caption"><span class="caption-text">Code Examples:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="capture_recapture.html">Capture-Recapture Models (CJS Models)</a></li>
<li class="toctree-l1"><a class="reference internal" href="cevae.html">Causal Effect VAE</a></li>
<li class="toctree-l1"><a class="reference internal" href="hmm.html">Hidden Markov Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="lda.html">Latent Dirichlet Allocation</a></li>
<li class="toctree-l1"><a class="reference internal" href="mcmc.html">Markov Chain Monte Carlo</a></li>
<li class="toctree-l1"><a class="reference internal" href="neutra.html">NeuTraReparam</a></li>
<li class="toctree-l1"><a class="reference internal" href="sparse_gamma.html">Sparse Gamma Deep Exponential Family</a></li>
<li class="toctree-l1"><a class="reference internal" href="dkl.html">Deep Kernel Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="einsum.html">Plated Einsum</a></li>
<li class="toctree-l1"><a class="reference internal" href="forecast_simple.html">Multivariate Forecasting</a></li>
<li class="toctree-l1"><a class="reference internal" href="timeseries.html">Gaussian Process Time Series Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="smcfilter.html">Sequential Monte Carlo Filtering</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Pyro Tutorials 编译</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>Dirichlet Process Mixture Models in Pyro</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/dirichlet_process_mixture.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container,
div.nbinput.container div.prompt,
div.nbinput.container div.input_area,
div.nbinput.container div[class*=highlight],
div.nbinput.container div[class*=highlight] pre,
div.nboutput.container,
div.nboutput.container div.prompt,
div.nboutput.container div.output_area,
div.nboutput.container div[class*=highlight],
div.nboutput.container div[class*=highlight] pre {
    background: none;
    border: none;
    padding: 0 0;
    margin: 0;
    box-shadow: none;
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    min-width: 5ex;
    padding-top: 0.3rem;
    padding-right: 0.3rem;
    text-align: right;
    flex: 0;
}
@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    background: #f5f5f5;
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 0.3rem;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt a.copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="Dirichlet-Process-Mixture-Models-in-Pyro">
<h1>Dirichlet Process Mixture Models in Pyro<a class="headerlink" href="#Dirichlet-Process-Mixture-Models-in-Pyro" title="Permalink to this headline">¶</a></h1>
<div class="section" id="What-are-Bayesian-nonparametric-models?">
<h2>What are Bayesian nonparametric models?<a class="headerlink" href="#What-are-Bayesian-nonparametric-models?" title="Permalink to this headline">¶</a></h2>
<p>Bayesian nonparametric models are models where the number of parameters grow freely with the amount of data provided; thus, instead of training several models that vary in complexity and comparing them, one is able to design a model whose complexity grows as more data are observed. The prototypical example of Bayesian nonparametrics in practice is the <em>Dirichlet Process Mixture Model</em> (DPMM). A DPMM allows for a practitioner to build a mixture model when the number of distinct clusters in the
geometric structure of their data is unknown – in other words, the number of clusters is allowed to grow as more data is observed. This feature makes the DPMM highly useful towards exploratory data analysis, where few facets of the data in question are known; this presentation aims to demonstrate this fact.</p>
</div>
<div class="section" id="The-Dirichlet-Process-(Ferguson,-1973)">
<h2>The Dirichlet Process (Ferguson, 1973)<a class="headerlink" href="#The-Dirichlet-Process-(Ferguson,-1973)" title="Permalink to this headline">¶</a></h2>
<p>Dirichlet processes are a family of probability distributions over discrete probability distributions. Formally, the Dirichlet process (DP) is specified by some base probability distribution <span class="math notranslate nohighlight">\(G_0: \Omega \to \mathbb{R}\)</span> and a positive, real, scaling parameter commonly denoted as <span class="math notranslate nohighlight">\(\alpha\)</span>. A sample <span class="math notranslate nohighlight">\(G\)</span> from a Dirichlet process with parameters <span class="math notranslate nohighlight">\(G_0: \Omega \to \mathbb{R}\)</span> and <span class="math notranslate nohighlight">\(\alpha\)</span> is itself a distribution over <span class="math notranslate nohighlight">\(\Omega\)</span>. For any disjoint partition
<span class="math notranslate nohighlight">\(\Omega_1, ..., \Omega_k\)</span> of <span class="math notranslate nohighlight">\(\Omega\)</span>, and any sample <span class="math notranslate nohighlight">\(G \sim DP(G_0, \alpha)\)</span>, we have:</p>
<div class="math notranslate nohighlight">
\[(G(\Omega_1), ..., G(\Omega_k)) \sim \text{Dir}(\alpha G_0(\Omega_1), ..., \alpha G_0(\Omega_k))\]</div>
<p>Essentially, this is taking a discrete partition of our sample space <span class="math notranslate nohighlight">\(\Omega\)</span> and subsequently constructing a discrete distribution over it using the base distribution <span class="math notranslate nohighlight">\(G_0\)</span>. While quite abstract in formulation, the Dirichlet process is very useful as a prior in various graphical models. This fact becomes easier to see in the following scheme.</p>
</div>
<div class="section" id="The-Chinese-Restaurant-Process-(Aldous,-1985)">
<h2>The Chinese Restaurant Process (Aldous, 1985)<a class="headerlink" href="#The-Chinese-Restaurant-Process-(Aldous,-1985)" title="Permalink to this headline">¶</a></h2>
<p>Imagine a restaurant with infinite tables (indexed by the positive integers) that accepts customers one at a time. The <span class="math notranslate nohighlight">\(n\)</span>th customer chooses their seat according to the following probabilities:</p>
<ul class="simple">
<li><p>With probability <span class="math notranslate nohighlight">\(\frac{n_t}{\alpha + n - 1}\)</span>, sit at table <span class="math notranslate nohighlight">\(t\)</span>, where <span class="math notranslate nohighlight">\(n_t\)</span> is the number of people at table <span class="math notranslate nohighlight">\(t\)</span></p></li>
<li><p>With probability <span class="math notranslate nohighlight">\(\frac{\alpha}{\alpha + n - 1}\)</span>, sit at an empty table</p></li>
</ul>
<p>If we associate to each table <span class="math notranslate nohighlight">\(t\)</span> a draw from a base distribution <span class="math notranslate nohighlight">\(G_0\)</span> over <span class="math notranslate nohighlight">\(\Omega\)</span>, and then associate unnormalized probability mass <span class="math notranslate nohighlight">\(n_t\)</span> to that draw, the resulting distribution over <span class="math notranslate nohighlight">\(\Omega\)</span> is equivalent to a draw from a Dirichlet process <span class="math notranslate nohighlight">\(DP(G_0, \alpha)\)</span>.</p>
<p>Furthermore, we can easily extend this to define the generative process of a nonparametric mixture model: every table <span class="math notranslate nohighlight">\(t\)</span> that has at least one customer seated is associated with a set of cluster parameters <span class="math notranslate nohighlight">\(\theta_t\)</span>, which were themselves drawn from some base distribution <span class="math notranslate nohighlight">\(G_0\)</span>. For each new observation, first assign that observation to a table according to the above probabilities; then, that observation is drawn from the distribution parameterized by the cluster parameters
for that table. If the observation was assigned to a new table, draw a new set of cluster parameters from <span class="math notranslate nohighlight">\(G_0\)</span>, and then draw the observation from the distribution parameterized by those cluster parameters.</p>
<p>While this formulation of a Dirichlet process mixture model is intuitive, it is also very difficult to perform inference on in a probabilistic programming framework. This motivates an alternative formulation of DPMMs, which has empirically been shown to be more conducive to inference (e.g. Blei and Jordan, 2004).</p>
</div>
<div class="section" id="The-Stick-Breaking-Method-(Sethuraman,-1994)">
<h2>The Stick-Breaking Method (Sethuraman, 1994)<a class="headerlink" href="#The-Stick-Breaking-Method-(Sethuraman,-1994)" title="Permalink to this headline">¶</a></h2>
<p>The generative process for the stick-breaking formulation of DPMMs proceeds as follows:</p>
<ul class="simple">
<li><p>Draw <span class="math notranslate nohighlight">\(\beta_i \sim \text{Beta}(1, \alpha)\)</span> for <span class="math notranslate nohighlight">\(i \in \mathbb{N}\)</span></p></li>
<li><p>Draw <span class="math notranslate nohighlight">\(\theta_i \sim G_0\)</span> for <span class="math notranslate nohighlight">\(i \in \mathbb{N}\)</span></p></li>
<li><p>Construct the mixture weights <span class="math notranslate nohighlight">\(\pi\)</span> by taking <span class="math notranslate nohighlight">\(\pi_i(\beta_{1:\infty}) = \beta_i \prod_{j&lt;i} (1-\beta_j)\)</span></p></li>
<li><p>For each observation <span class="math notranslate nohighlight">\(n \in \{1, ..., N\}\)</span>, draw <span class="math notranslate nohighlight">\(z_n \sim \pi(\beta_{1:\infty})\)</span>, and then draw <span class="math notranslate nohighlight">\(x_n \sim f(\theta_{z_n})\)</span></p></li>
</ul>
<p>Here, the infinite nature of the Dirichlet process mixture model can more easily be seen. Furthermore, all <span class="math notranslate nohighlight">\(\beta_i\)</span> are independent, so it is far easier to perform inference in a probabilistic programming framework.</p>
<p>First, we import all the modules we’re going to need:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>import torch
from torch.distributions import constraints
import torch.nn.functional as F
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.cm as cm
from tqdm import tqdm

from pyro.distributions import *
import pyro
from pyro.optim import Adam
from pyro.infer import SVI, Trace_ELBO, Predictive
assert pyro.__version__.startswith(&#39;1&#39;)
pyro.enable_validation(True)       # can help with debugging
pyro.set_rng_seed(0)
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Collecting pyro-ppl
  Downloading https://files.pythonhosted.org/packages/75/c8/3a52883ab27c5503385a983e4451a1e2944c2baab66c0f7ffc1f84bca329/pyro_ppl-1.1.0-py3-none-any.whl (429kB)
     |████████████████████████████████| 430kB 3.4MB/s
Collecting pyro-api&gt;=0.1.1
  Downloading https://files.pythonhosted.org/packages/c2/bc/6cdbd1929e32fff62a33592633c2cc0393c7f7739131ccc9c9c4e28ac8dd/pyro_api-0.1.1-py3-none-any.whl
Requirement already satisfied: torch&gt;=1.3.0 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (1.3.1)
Collecting tqdm&gt;=4.36
  Downloading https://files.pythonhosted.org/packages/7f/32/5144caf0478b1f26bd9d97f510a47336cf4ac0f96c6bc3b5af20d4173920/tqdm-4.40.2-py2.py3-none-any.whl (55kB)
     |████████████████████████████████| 61kB 4.2MB/s
Requirement already satisfied: opt-einsum&gt;=2.3.2 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (3.1.0)
Requirement already satisfied: numpy&gt;=1.7 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (1.17.4)
Installing collected packages: pyro-api, tqdm, pyro-ppl
  Found existing installation: tqdm 4.28.1
    Uninstalling tqdm-4.28.1:
      Successfully uninstalled tqdm-4.28.1
Successfully installed pyro-api-0.1.1 pyro-ppl-1.1.0 tqdm-4.40.2
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="admonition warning">
<p>Data type cannot be displayed: application/vnd.colab-display-data+json</p>
</div>
</div>
</div>
</div>
<div class="section" id="Inference">
<h2><strong>Inference</strong><a class="headerlink" href="#Inference" title="Permalink to this headline">¶</a></h2>
<div class="section" id="Synthetic-Mixture-of-Gaussians">
<h3>Synthetic Mixture of Gaussians<a class="headerlink" href="#Synthetic-Mixture-of-Gaussians" title="Permalink to this headline">¶</a></h3>
<p>We begin by demonstrating the capabilities of Dirichlet process mixture models on a synthetic dataset generated by a mixture of four 2D Gaussians:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>data = torch.cat((MultivariateNormal(-8 * torch.ones(2), torch.eye(2)).sample([50]),
                  MultivariateNormal(8 * torch.ones(2), torch.eye(2)).sample([50]),
                  MultivariateNormal(torch.tensor([1.5, 2]), torch.eye(2)).sample([50]),
                  MultivariateNormal(torch.tensor([-0.5, 1]), torch.eye(2)).sample([50])))

plt.scatter(data[:, 0], data[:, 1])
plt.title(&quot;Data Samples from Mixture of 4 Gaussians&quot;)
plt.show()
N = data.shape[0]
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/dirichlet_process_mixture_3_0.png" src="_images/dirichlet_process_mixture_3_0.png" />
</div>
</div>
<p>In this example, the cluster parameters <span class="math notranslate nohighlight">\(\theta_i\)</span> are two dimensional vectors describing the means of a multivariate Gaussian with identity covariance. Therefore, the Dirichlet process base distribution <span class="math notranslate nohighlight">\(G_0\)</span> is also a multivariate Gaussian (i.e. the conjugate prior), although this choice is not as computationally useful, since we are not performing coordinate-ascent variational inference but rather black-box variational inference using Pyro.</p>
<p>First, let’s define the “stick-breaking” function that generates our weights, given our samples of <span class="math notranslate nohighlight">\(\beta\)</span>:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>def mix_weights(beta):
    beta1m_cumprod = (1 - beta).cumprod(-1)
    return F.pad(beta, (0, 1), value=1) * F.pad(beta1m_cumprod, (1, 0), value=1)
</pre></div>
</div>
</div>
<p>Next, let’s define our model. It may be helpful to refer the definition of the stick-breaking model presented in the first part of this tutorial.</p>
<p>Note that all <span class="math notranslate nohighlight">\(\beta_i\)</span> samples are conditionally independent, so we model them using a <code class="docutils literal notranslate"><span class="pre">pyro.plate</span></code> of size <code class="docutils literal notranslate"><span class="pre">T-1</span></code>; we do the same for all samples of our cluster parameters <span class="math notranslate nohighlight">\(\mu_i\)</span>. We then construct a Categorical distribution whose parameters are the mixture weights using our sampled <span class="math notranslate nohighlight">\(\beta\)</span> values (line 9) below, and sample the cluster assignment <span class="math notranslate nohighlight">\(z_n\)</span> for each data point from that Categorical. Finally, we sample our observations from a multivariate Gaussian
distribution whose mean is exactly the cluster parameter corresponding to the assignment <span class="math notranslate nohighlight">\(z_n\)</span> we drew for the point <span class="math notranslate nohighlight">\(x_n\)</span>. This can be seen in the Pyro code below:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>def model(data):
    with pyro.plate(&quot;beta_plate&quot;, T-1):
        beta = pyro.sample(&quot;beta&quot;, Beta(1, alpha))

    with pyro.plate(&quot;mu_plate&quot;, T):
        mu = pyro.sample(&quot;mu&quot;, MultivariateNormal(torch.zeros(2), 5 * torch.eye(2)))

    with pyro.plate(&quot;data&quot;, N):
        z = pyro.sample(&quot;z&quot;, Categorical(mix_weights(beta)))
        pyro.sample(&quot;obs&quot;, MultivariateNormal(mu[z], torch.eye(2)), obs=data)
</pre></div>
</div>
</div>
<p>Now, it’s time to define our guide and perform inference.</p>
<p>The variational family <span class="math notranslate nohighlight">\(q(\beta, \theta, z)\)</span> that we are optimizing over during variational inference is given by:</p>
<div class="math notranslate nohighlight">
\[q(\beta, \theta, z) = \prod_{t=1}^{T-1} q_t(\beta_t) \prod_{t=1}^T q_t(\theta_t) \prod_{n=1}^N q_n(z_n)\]</div>
<p>Note that since we are unable to computationally model the infinite clusters posited by the model, we truncate our variational family at <span class="math notranslate nohighlight">\(T\)</span> clusters. This does not affect our model; rather, it is a simplification made in the <em>inference</em> stage to allow tractability.</p>
<p>The guide is constructed exactly according to the definition of our variational family <span class="math notranslate nohighlight">\(q(\beta, \theta, z)\)</span> above. We have <span class="math notranslate nohighlight">\(T-1\)</span> conditionally independent Beta distributions for each <span class="math notranslate nohighlight">\(\beta\)</span> sampled in our model, <span class="math notranslate nohighlight">\(T\)</span> conditionally independent multivariate Gaussians for each cluster parameter <span class="math notranslate nohighlight">\(\mu_i\)</span>, and <span class="math notranslate nohighlight">\(N\)</span> conditionally independent Categorical distributions for each cluster assignment <span class="math notranslate nohighlight">\(z_n\)</span>.</p>
<p>Our variational parameters (<code class="docutils literal notranslate"><span class="pre">pyro.param</span></code>) are therefore the <span class="math notranslate nohighlight">\(T-1\)</span> many positive scalars that parameterize the second parameter of our variational Beta distributions (the first shape parameter is fixed at <span class="math notranslate nohighlight">\(1\)</span>, as in the model definition), the <span class="math notranslate nohighlight">\(T\)</span> many two-dimensional vectors that parameterize our variational multivariate Gaussian distributions (we do not parameterize the covariance matrices of the Gaussians, though this should be done when analyzing a real-world dataset for
more flexibility), and the <span class="math notranslate nohighlight">\(N\)</span> many <span class="math notranslate nohighlight">\(T\)</span>-dimensional vectors that parameterize our variational Categorical distributions:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>def guide(data):
    kappa = pyro.param(&#39;kappa&#39;, lambda: Uniform(0, 2).sample([T-1]), constraint=constraints.positive)
    tau = pyro.param(&#39;tau&#39;, lambda: MultivariateNormal(torch.zeros(2), 3 * torch.eye(2)).sample([T]))
    phi = pyro.param(&#39;phi&#39;, lambda: Dirichlet(1/T * torch.ones(T)).sample([N]), constraint=constraints.simplex)

    with pyro.plate(&quot;beta_plate&quot;, T-1):
        q_beta = pyro.sample(&quot;beta&quot;, Beta(torch.ones(T-1), kappa))

    with pyro.plate(&quot;mu_plate&quot;, T):
        q_mu = pyro.sample(&quot;mu&quot;, MultivariateNormal(tau, torch.eye(2)))

    with pyro.plate(&quot;data&quot;, N):
        z = pyro.sample(&quot;z&quot;, Categorical(phi))
</pre></div>
</div>
</div>
<p>When performing inference, we set our ‘guess’ for the maximum number of clusters in the dataset to <span class="math notranslate nohighlight">\(T = 6\)</span>. We define the optimization algorithm (<code class="docutils literal notranslate"><span class="pre">pyro.optim.Adam</span></code>) along with the Pyro SVI object and train the model for 1000 iterations.</p>
<p>After performing inference, we construct the Bayes estimators of the means (the expected values of each factor in our variational approximation) and plot them in red on top of the original dataset. Note that we also have we removed any clusters that have less than a certain weight assigned to them according to our learned variational distributions, and then re-normalize the weights so that they sum to one:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>T = 6
optim = Adam({&quot;lr&quot;: 0.05})
svi = SVI(model, guide, optim, loss=Trace_ELBO())
losses = []

def train(num_iterations):
    pyro.clear_param_store()
    for j in tqdm(range(num_iterations)):
        loss = svi.step(data)
        losses.append(loss)

def truncate(alpha, centers, weights):
    threshold = alpha**-1 / 100.
    true_centers = centers[weights &gt; threshold]
    true_weights = weights[weights &gt; threshold] / torch.sum(weights[weights &gt; threshold])
    return true_centers, true_weights

alpha = 0.1
train(1000)

# We make a point-estimate of our model parameters using the posterior means of tau and phi for the centers and weights
Bayes_Centers_01, Bayes_Weights_01 = truncate(alpha, pyro.param(&quot;tau&quot;).detach(), torch.mean(pyro.param(&quot;phi&quot;).detach(), dim=0))

alpha = 1.5
train(1000)

# We make a point-estimate of our model parameters using the posterior means of tau and phi for the centers and weights
Bayes_Centers_15, Bayes_Weights_15 = truncate(alpha, pyro.param(&quot;tau&quot;).detach(), torch.mean(pyro.param(&quot;phi&quot;).detach(), dim=0))

plt.figure(figsize=(15, 5))
plt.subplot(1, 2, 1)
plt.scatter(data[:, 0], data[:, 1], color=&quot;blue&quot;)
plt.scatter(Bayes_Centers_01[:, 0], Bayes_Centers_01[:, 1], color=&quot;red&quot;)

plt.subplot(1, 2, 2)
plt.scatter(data[:, 0], data[:, 1], color=&quot;blue&quot;)
plt.scatter(Bayes_Centers_15[:, 0], Bayes_Centers_15[:, 1], color=&quot;red&quot;)
plt.tight_layout()
plt.show()
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
100%|██████████| 1000/1000 [00:06&lt;00:00, 153.93it/s]
100%|██████████| 1000/1000 [00:06&lt;00:00, 152.39it/s]
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/dirichlet_process_mixture_11_1.png" src="_images/dirichlet_process_mixture_11_1.png" />
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
&lt;Figure size 432x288 with 0 Axes&gt;
</pre></div></div>
</div>
<p>The plots above demonstrate the effects of the scaling hyperparameter <span class="math notranslate nohighlight">\(\alpha\)</span>. A greater <span class="math notranslate nohighlight">\(\alpha\)</span> yields a more heavy-tailed distribution of the weights, whereas smaller <span class="math notranslate nohighlight">\(\alpha\)</span> will place more mass on fewer clusters. In particular, the middle cluster looks like it could be generated a single Gaussian (although in fact it was generated by two distinct Gaussians), and thus the setting of <span class="math notranslate nohighlight">\(\alpha\)</span> allows the practitioner to further encode their prior beliefs about how
many clusters the data contains.</p>
</div>
<div class="section" id="Dirichlet-Mixture-Model-for-Long-Term-Solar-Observations">
<h3>Dirichlet Mixture Model for Long Term Solar Observations<a class="headerlink" href="#Dirichlet-Mixture-Model-for-Long-Term-Solar-Observations" title="Permalink to this headline">¶</a></h3>
<p>As mentioned earlier, the Dirichlet process mixture model truly shines when exploring a dataset whose latent geometric structure is completely unknown. To demonstrate this, we fit a DPMM on sunspot count data taken over the past 300 years (provided by the Royal Observatory of Belgium):</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>df = pd.read_csv(&#39;http://www.sidc.be/silso/DATA/SN_y_tot_V2.0.csv&#39;, sep=&#39;;&#39;, names=[&#39;time&#39;, &#39;sunspot.year&#39;], usecols=[0, 1])
data = torch.tensor(df[&#39;sunspot.year&#39;].values, dtype=torch.float32)
N = data.shape[0]

plt.hist(df[&#39;sunspot.year&#39;].values, bins=40)
plt.title(&quot;Number of Years vs. Sunspot Counts&quot;)
plt.xlabel(&quot;Sunspot Count&quot;)
plt.ylabel(&quot;Number of Years&quot;)
plt.show()
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/dirichlet_process_mixture_14_0.png" src="_images/dirichlet_process_mixture_14_0.png" />
</div>
</div>
<p>For this example, the cluster parameters <span class="math notranslate nohighlight">\(\theta_i\)</span> are rate parameters since we are constructing a scale-mixture of Poisson distributions. Again, <span class="math notranslate nohighlight">\(G_0\)</span> is chosen to be the conjugate prior, which in this case is a Gamma distribution, though this still does not strictly matter for doing inference through Pyro. Below is the implementation of the model:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>def model(data):
    with pyro.plate(&quot;beta_plate&quot;, T-1):
        beta = pyro.sample(&quot;beta&quot;, Beta(1, alpha))

    with pyro.plate(&quot;lambda_plate&quot;, T):
        lmbda = pyro.sample(&quot;lambda&quot;, Gamma(3, 0.05))

    with pyro.plate(&quot;data&quot;, N):
        z = pyro.sample(&quot;z&quot;, Categorical(mix_weights(beta)))
        pyro.sample(&quot;obs&quot;, Poisson(lmbda[z]), obs=data)

def guide(data):
    kappa = pyro.param(&#39;kappa&#39;, lambda: Uniform(0, 2).sample([T-1]), constraint=constraints.positive)
    tau_0 = pyro.param(&#39;tau_0&#39;, lambda: Uniform(0, 5).sample([T]), constraint=constraints.positive)
    tau_1 = pyro.param(&#39;tau_1&#39;, lambda: LogNormal(-1, 1).sample([T]), constraint=constraints.positive)
    phi = pyro.param(&#39;phi&#39;, lambda: Dirichlet(1/T * torch.ones(T)).sample([N]), constraint=constraints.simplex)

    with pyro.plate(&quot;beta_plate&quot;, T-1):
        q_beta = pyro.sample(&quot;beta&quot;, Beta(torch.ones(T-1), kappa))

    with pyro.plate(&quot;lambda_plate&quot;, T):
        q_lambda = pyro.sample(&quot;lambda&quot;, Gamma(tau_0, tau_1))

    with pyro.plate(&quot;data&quot;, N):
        z = pyro.sample(&quot;z&quot;, Categorical(phi))

T = 20
alpha = 1.1
n_iter = 1500
optim = Adam({&quot;lr&quot;: 0.05})
svi = SVI(model, guide, optim, loss=Trace_ELBO())
losses = []

train(n_iter)

samples = torch.arange(0, 300).type(torch.float)

tau0_optimal = pyro.param(&quot;tau_0&quot;).detach()
tau1_optimal = pyro.param(&quot;tau_1&quot;).detach()
kappa_optimal = pyro.param(&quot;kappa&quot;).detach()

# We make a point-estimate of our latent variables using the posterior means of tau and kappa for the cluster params and weights
Bayes_Rates = (tau0_optimal / tau1_optimal)
Bayes_Weights = mix_weights(1. / (1. + kappa_optimal))

def mixture_of_poisson(weights, rates, samples):
    return (weights * Poisson(rates).log_prob(samples.unsqueeze(-1)).exp()).sum(-1)

likelihood = mixture_of_poisson(Bayes_Weights, Bayes_Rates, samples)

plt.title(&quot;Number of Years vs. Sunspot Counts&quot;)
plt.hist(data, bins=60, density=True, lw=0, alpha=0.75);
plt.plot(samples, likelihood, label=&quot;Estimated Mixture Density&quot;)
plt.legend()
plt.show()
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
100%|██████████| 1500/1500 [00:09&lt;00:00, 160.10it/s]
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/dirichlet_process_mixture_16_1.png" src="_images/dirichlet_process_mixture_16_1.png" />
</div>
</div>
<p>The above plot is the mixture density of the Bayes estimators of the cluster parameters, weighted by their corresponding weights. As in the Gaussian example, we have taken the Bayes estimators of each cluster parameter and their corresponding weights by computing the posterior means of <code class="docutils literal notranslate"><span class="pre">lambda</span></code> and <code class="docutils literal notranslate"><span class="pre">beta</span></code> respectively.</p>
</div>
<div class="section" id="ELBO-Behavior">
<h3>ELBO Behavior<a class="headerlink" href="#ELBO-Behavior" title="Permalink to this headline">¶</a></h3>
<p>Below are plots of the behavior of the loss function (negative Trace_ELBO) over the SVI iterations during inference using Pyro, as well as a plot of the autocorrelations of the ELBO ‘time series’ versus iteration number. We can see that around 500 iterations, the loss stops decreasing significantly, so we can assume it takes around 500 iterations to achieve convergence. The autocorrelation plot reaches an autocorrelation very close to 0 around a lag of 500, further corroborating this hypothesis.
Note that these are heuristics and do not necessarily imply convergence.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>elbo_plot = plt.figure(figsize=(15, 5))

elbo_ax = elbo_plot.add_subplot(1, 2, 1)
elbo_ax.set_title(&quot;ELBO Value vs. Iteration Number for Pyro BBVI on Sunspot Data&quot;)
elbo_ax.set_ylabel(&quot;ELBO&quot;)
elbo_ax.set_xlabel(&quot;Iteration Number&quot;)
elbo_ax.plot(np.arange(n_iter), losses)

autocorr_ax = elbo_plot.add_subplot(1, 2, 2)
autocorr_ax.acorr(np.asarray(losses), detrend=lambda x: x - x.mean(), maxlags=750, usevlines=False, marker=&#39;,&#39;)
autocorr_ax.set_xlim(0, 500)
autocorr_ax.axhline(0, ls=&quot;--&quot;, c=&quot;k&quot;, lw=1)
autocorr_ax.set_title(&quot;Autocorrelation of ELBO vs. Lag for Pyro BBVI on Sunspot Data&quot;)
autocorr_ax.set_xlabel(&quot;Lag&quot;)
autocorr_ax.set_ylabel(&quot;Autocorrelation&quot;)
elbo_plot.tight_layout()
elbo_plot.show()

</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/dirichlet_process_mixture_19_0.png" src="_images/dirichlet_process_mixture_19_0.png" />
</div>
</div>
</div>
</div>
<div class="section" id="Criticism">
<h2><strong>Criticism</strong><a class="headerlink" href="#Criticism" title="Permalink to this headline">¶</a></h2>
<div class="section" id="Long-Term-Sunspot-Model">
<h3>Long-Term Sunspot Model<a class="headerlink" href="#Long-Term-Sunspot-Model" title="Permalink to this headline">¶</a></h3>
<p>Since we computed the approximate posterior of the DPMM that was fit to the long-term sunspot data, we can utilize some intrinsic metrics, such as the log predictive, posterior dispersion indices, and posterior predictive checks.</p>
<p>Since the posterior predictive distribution for a Dirichlet process mixture model is itself a scale-mixture distribution that has an analytic approximation <a class="reference external" href="http://www.cs.columbia.edu/~blei/papers/BleiJordan2004.pdf">(Blei and Jordan, 2004)</a>, this makes it particularly amenable to the aforementioned metrics:</p>
<div class="math notranslate nohighlight">
\[p(x_{new} | X_{1:N}, \alpha, G_0) \approx \sum_{t=1}^T \mathbb{E}_q [\pi_t(\beta)] \ \mathbb{E}_q \left[p(x_{new} | \theta_t)\right].\]</div>
<p>In particular, to compute the log predictive, we first compute the posterior predictive distribution (defined above) after performing variational inference on our model using a training subsample of our data. The log predictive is then the log value of the predictive density evaluated at each point in the test subsample:</p>
<div class="math notranslate nohighlight">
\[\log p(x_{new} | X) = \log \mathbb{E}_{\beta, \theta | X} \left[ p(x_{new} | \beta, \theta) \right]\]</div>
<p>Since both the training samples and the testing samples were taken from the same dataset, we would expect the model to assign high probability to the test samples, despite not having seen them during inference. This gives a metric by which to select values of <span class="math notranslate nohighlight">\(T\)</span>, <span class="math notranslate nohighlight">\(\alpha\)</span>, and <span class="math notranslate nohighlight">\(G_0\)</span>, our hyperparameters: we would want to choose the values that maximize this value.</p>
<p>We perform this process below with varying values of <span class="math notranslate nohighlight">\(\alpha\)</span> to see what the optimal setting is.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># Hold out 10% of our original data to test upon
df_test = df.sample(frac=0.1)
data = torch.tensor(df.drop(df_test.index)[&#39;sunspot.year&#39;].values, dtype=torch.float)
data_test = torch.tensor(df_test[&#39;sunspot.year&#39;].values, dtype=torch.float)
N = data.shape[0]
N_test = data_test.shape[0]

alphas = [0.05, 0.1, 0.5, 0.75, 0.9, 1., 1.25, 1.5, 2, 2.5, 3]
log_predictives = []

for val in alphas:
    alpha = val
    T = 20
    svi = SVI(model, guide, optim, loss=Trace_ELBO())
    train(500)

    S = 100 # number of Monte Carlo samples to use in posterior predictive computations

    # Using pyro&#39;s built in posterior predictive class:
    posterior = Predictive(guide, num_samples=S, return_sites=[&quot;beta&quot;, &quot;lambda&quot;])(data)
    post_pred_weights = mix_weights(posterior[&quot;beta&quot;])
    post_pred_clusters = posterior[&quot;lambda&quot;]

    # log_prob shape = N_test x S
    log_prob = (post_pred_weights.log() + Poisson(post_pred_clusters).log_prob(data.reshape(-1, 1, 1))).logsumexp(-1)
    mean_log_prob = log_prob.logsumexp(-1) - np.log(S)
    log_posterior_predictive = mean_log_prob.sum(-1)
    log_predictives.append(log_posterior_predictive)

plt.figure(figsize=(10, 5))
plt.plot(alphas, log_predictives)
plt.title(&quot;Value of the Log Predictive at Varying Alpha&quot;)
plt.show()
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
100%|██████████| 500/500 [00:03&lt;00:00, 163.51it/s]
100%|██████████| 500/500 [00:03&lt;00:00, 164.64it/s]
100%|██████████| 500/500 [00:02&lt;00:00, 169.77it/s]
100%|██████████| 500/500 [00:02&lt;00:00, 170.06it/s]
100%|██████████| 500/500 [00:02&lt;00:00, 171.63it/s]
100%|██████████| 500/500 [00:02&lt;00:00, 173.58it/s]
100%|██████████| 500/500 [00:02&lt;00:00, 173.57it/s]
100%|██████████| 500/500 [00:02&lt;00:00, 175.42it/s]
100%|██████████| 500/500 [00:02&lt;00:00, 176.11it/s]
100%|██████████| 500/500 [00:02&lt;00:00, 175.92it/s]
100%|██████████| 500/500 [00:02&lt;00:00, 175.63it/s]
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/dirichlet_process_mixture_22_1.png" src="_images/dirichlet_process_mixture_22_1.png" />
</div>
</div>
<p>From the above plot, we would surmise that we want to set <span class="math notranslate nohighlight">\(\alpha &gt; 1\)</span>, though the signal is not quite clear. A more comprehensive model criticism process would involve performing a grid search across all hyperparameters in order to find the one that maximizes the log predictive.</p>
</div>
</div>
<div class="section" id="References">
<h2>References<a class="headerlink" href="#References" title="Permalink to this headline">¶</a></h2>
<ol class="arabic simple">
<li><p>Ferguson, Thomas. <em>A Bayesian Analysis of Some Nonparametric Problems</em>. The Annals of Statistics, Vol. 1, No. 2 (1973).</p></li>
<li><p>Aldous, D. <em>Exchangeability and Related Topics</em>. Ecole diete de Probabilities Saint Flour (1985).</p></li>
<li><p>Sethuraman, J. <em>A Constructive Definition of Dirichlet Priors</em>. Statistica, Sinica, 4:639-650 (1994).</p></li>
<li><p>Blei, David and Jordan, Michael. <em>Variational Inference for Dirichlet Process Mixtures</em>. Bayesian Analysis, Vol. 1, No. 1 (2004).</p></li>
<li><p>Pedregosa, et al. <em>Scikit-Learn: Machine Learning in Python</em>. JMLR 12, pp. 2825-2830 (2011).</p></li>
<li><p>Bishop, Christopher. <em>Pattern Recogition and Machine Learning</em>. Springer Ltd (2006).</p></li>
<li><p><em>Sunspot Index and Long-Term Solar Observations</em>. WDC-SILSO, Royal Observatory of Belgium, Brussels (2018).</p></li>
<li><p>Gelman, Andrew. <em>Understanding predictive information criteria for Bayesian models</em>. Statistics and Computing, Springer Link, 2014.</p></li>
</ol>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="boosting_bbvi.html" class="btn btn-neutral float-right" title="Boosting Black Box Variational Inference" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="elections.html" class="btn btn-neutral float-left" title="Predicting the outcome of a US presidential election using Bayesian optimal experimental design" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright Uber Technologies, Inc; 编译 by Heyang Gong

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>